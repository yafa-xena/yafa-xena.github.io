<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Hadoop - Yafa Xena's blog</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="Yafa Xena"><meta name=description content="前言 最近准备接手一些大数据的东西了，在接手之前肯定是需要对大数据的生态以及常用的软件配置使用等等都要有一个比较基础的认知，为此我会写几篇文章关于大数据的那些事情，这篇文章就是关于hadoop的。"><meta name=keywords content="Linux,Emacs,Python,Golang,Devops,Zabbix,Kubernetes,Gentoo"><meta name=generator content="Hugo 0.79.1 with theme even"><link rel=canonical href=https://www.yafa.moe/post/hadoop/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link href=/sass/main.min.b5a744db6de49a86cadafb3b70f555ab443f83c307a483402259e94726b045ff.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="Hadoop"><meta property="og:description" content="前言
最近准备接手一些大数据的东西了，在接手之前肯定是需要对大数据的生态以及常用的软件配置使用等等都要有一个比较基础的认知，为此我会写几篇文章关于大数据的那些事情，这篇文章就是关于hadoop的。"><meta property="og:type" content="article"><meta property="og:url" content="https://www.yafa.moe/post/hadoop/"><meta property="article:published_time" content="2021-02-01T16:51:57+08:00"><meta property="article:modified_time" content="2021-02-01T16:51:57+08:00"><meta itemprop=name content="Hadoop"><meta itemprop=description content="前言
最近准备接手一些大数据的东西了，在接手之前肯定是需要对大数据的生态以及常用的软件配置使用等等都要有一个比较基础的认知，为此我会写几篇文章关于大数据的那些事情，这篇文章就是关于hadoop的。"><meta itemprop=datePublished content="2021-02-01T16:51:57+08:00"><meta itemprop=dateModified content="2021-02-01T16:51:57+08:00"><meta itemprop=wordCount content="11325"><meta itemprop=keywords content="hadoop,linux,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Hadoop"><meta name=twitter:description content="前言
最近准备接手一些大数据的东西了，在接手之前肯定是需要对大数据的生态以及常用的软件配置使用等等都要有一个比较基础的认知，为此我会写几篇文章关于大数据的那些事情，这篇文章就是关于hadoop的。"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>Yafa-Xena</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/todo><li class=mobile-menu-item>Todo</li></a><a href=/about><li class=mobile-menu-item>About</li></a><a href=/product><li class=mobile-menu-item>Product</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>Yafa-Xena</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/todo>Todo</a></li><li class=menu-item><a class=menu-item-link href=/about>About</a></li><li class=menu-item><a class=menu-item-link href=/product>Product</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>Hadoop</h1><div class=post-meta><span class=post-time>2021-02-01</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#前言>前言</a></li><li><a href=#hadoop介绍>hadoop介绍</a></li><li><a href=#高可用原理介绍>高可用原理介绍</a><ul><li><a href=#hdfs-ha原理>HDFS HA原理</a></li></ul></li><li><a href=#软件兼容表>软件兼容表</a></li><li><a href=#环境准备>环境准备</a><ul><li><a href=#节点配置>节点配置</a></li><li><a href=#集群规划>集群规划</a></li><li><a href=#端口说明>端口说明</a></li><li><a href=#虚拟机网络>虚拟机网络</a></li><li><a href=#添加管理用户>添加管理用户</a></li><li><a href=#创建上传包目录>创建上传包目录</a></li><li><a href=#ntp配置>NTP配置</a></li><li><a href=#安装jdk环境>安装JDK环境</a></li><li><a href=#上传包>上传包</a></li></ul></li><li><a href=#安装和zookeeper>安装和zookeeper</a><ul><li><a href=#安装zookeeper>安装zookeeper</a></li><li><a href=#配置zookeeper>配置zookeeper</a></li><li><a href=#启动zookeeper并检查状态>启动zookeeper并检查状态</a></li></ul></li><li><a href=#安装hadoop>安装hadoop</a><ul><li><a href=#免密>免密</a></li><li><a href=#下载hadoop>下载hadoop</a></li><li><a href=#启动>启动</a></li><li><a href=#hadoop-服务-systemd-service-文件>hadoop 服务 systemd service 文件</a></li><li><a href=#hdfs-测试>HDFS 测试</a></li><li><a href=#ha验证>HA验证</a></li><li><a href=#为每个服务编写systemd-services-文件>为每个服务编写systemd services 文件</a></li></ul></li><li><a href=#hive>hive</a></li><li><a href=#hbase>hbase</a></li><li><a href=#spark>Spark</a></li><li><a href=#海豚调度>海豚调度</a><ul><li><a href=#安装和配置海豚调度>安装和配置海豚调度</a></li></ul></li><li><a href=#监控>监控</a></li><li><a href=#日志处理>日志处理</a></li><li><a href=#后记>后记</a></li><li><a href=#参考链接>参考链接</a></li></ul></li></ul></nav></div></div><div class=post-content><h2 id=前言>前言</h2><p>最近准备接手一些大数据的东西了，在接手之前肯定是需要对大数据的生态以及常用的软件配置使用等等都要有一个比较基础的认知，为此我会写几篇文章关于大数据的那些事情，这篇文章就是关于hadoop的。</p><h2 id=hadoop介绍>hadoop介绍</h2><p>hadoop 是apache基金会开源出来的并行处理工具
Hbase：十一个nosql的数据库，类似于mongodb
HDFS： hadoop distribut file system, hadoop的分布式文件系统
Zookeeper是分布式管理协助框架，Zookeeper集群在这里用于保证Hadoop集群的高可用。</p><h2 id=高可用原理介绍>高可用原理介绍</h2><p>Zookeeper集群能够保证NameNode服务高可用省得原理是：Hadoop集群中有2个NameNode服务，两个Namenode服务都定时给Zookeeper发送心跳，告诉Zookeeper我还或者，可以提供服务，单个时间点只有一个是Action的状态，另一个是Standby状态，一旦Zookeeper检测不到Action NameNode发送来的心跳之后，就会切换到Standby状态的NameNode上，将它设置为Action NameNode的状态，以此来达到NameNode高可用的目的。</p><p>Zookeeper集群本身也可以保证自身的高可用，Zookeeper集群中的各个节点分为Leader、Follower两个。</p><p>当写数据的时候需要先写入Leader节点，Leader写入之后再通知Follower写入。</p><p>客户端读取数据的时候，因为数据都是一样的，可以从任意一台机器上进行读取数据。</p><p>Zookeeper当Leader节点发生故障的时候，就会进行选举流程。这个流程是：集群中任何一台机器发现集群中没有Leader的时候，就会推荐自己为Leader，其他机器来发起投票同意，当超过一半的机器同意它为Leader的时候，选举结束。</p><p>所以Zookeeper集群中的机器数量必须是奇数这样就算是Leader拒绝服务也会很快选出新的Leader，从而保证了Zookeeper集群的高可用性。</p><p>ZK的三个角色</p><ul><li>Leader</li><li>Follower</li><li>Observer</li></ul><h3 id=hdfs-ha原理>HDFS HA原理</h3><p>单点的NameNode的缺陷在于单点故障的问题，如果NameNode不可用会导致整个HDFS系统不可用，所以需要设计高可用的HDFS来解决NamNode单点故障的问题。及觉得方法就是再HDFS集群中设置多个NameNode节点，但是一旦引入多个NameNode就有一些问题需要解决。</p><ul><li>如何保证NameNode内存中元数据一致，并保证编辑日志文件的安全性。</li><li>多个NameNode如何进行协作</li><li>客户端如何能够找到可用的NameNode</li><li>如何保证任意时刻只有一个NameNode处于对外服务的状态。</li></ul><p>以下是引入ZK的解决方法</p><ul><li>对于保证NameNode元数据的一致性和编辑日志的安全性，采用Zookeeper来存储日志文件。</li><li>两个NameNode一个是Active状态的，一个是Standby状态的，一个时间点上之能够有一个为Active状态的NameNode。</li></ul><h2 id=软件兼容表>软件兼容表</h2><p>大数据的生态比较复杂，像是常用的hbase、hive都是对安装的软件是有些要求的，这里我总结了一个表格方便去查看和使用。以避免因为版本不兼容带来的各种各样的问题。</p><p>hadoop以及必要的组件版本如下：</p><table><thead><tr><th>软件名</th><th>版本</th><th>备注</th></tr></thead><tbody><tr><td>hadoop</td><td>3.3.0</td><td>stable</td></tr><tr><td>zookeeper</td><td>3.6.3</td><td>stable</td></tr><tr><td>hive</td><td>3.1.2</td><td>stable</td></tr><tr><td>hbase</td><td>2.3.5</td><td>stable</td></tr></tbody></table><h2 id=环境准备>环境准备</h2><p>这里准备了3台虚拟机来做这次的实验：</p><table><thead><tr><th>主机名</th><th>OS</th><th>ip</th></tr></thead><tbody><tr><td>hadoop-node1</td><td>CentOS7</td><td>192.168.122.40</td></tr><tr><td>hadoop-node2</td><td>CentOS7</td><td>192.168.122.41</td></tr><tr><td>hadoop-node3</td><td>CentOS7</td><td>192.168.122.42</td></tr></tbody></table><p>海豚调度版本如下：</p><table><thead><tr><th>软件名</th><th>版本</th><th>备注</th></tr></thead><tbody><tr><td>apache-dolphinscheduler</td><td>1.3.6</td><td>latest-bin</td></tr></tbody></table><p>角色描述：</p><table><thead><tr><th>描述</th><th>hadoop-node1</th><th>hadoop-node2</th><th>hadoop-node3</th></tr></thead><tbody><tr><td>HDFS主</td><td>NameNode</td><td>NameNode</td><td></td></tr><tr><td>HDFS从</td><td>DateNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td>Yarn主</td><td>ResourceManager</td><td>Resourcemanager</td><td></td></tr><tr><td>Yarn从</td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td>Hbase主</td><td>HMaster</td><td>Hmaster</td><td></td></tr><tr><td>Hbase从</td><td>HRegionServer</td><td>HRegionserver</td><td>HRegionserver</td></tr><tr><td>Zookeeper进程</td><td>QuorumPeerMain</td><td>Quorumpeermain</td><td>Quorumpeermain</td></tr><tr><td>NameNode数据同步</td><td>JournalNode</td><td>JournalNode</td><td>Journalnode</td></tr><tr><td>主备切换</td><td>DFSZKFailoverController</td><td>DFSzkfailovercontroller</td><td></td></tr></tbody></table><h3 id=节点配置>节点配置</h3><h4 id=关闭ipv6>关闭ipv6</h4><p>在<code>/etc/sysctl.conf</code>文件后追加内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>net.ipv6.conf.all.disable_ipv6<span class=o>=</span><span class=m>1</span>
net.ipv6.conf.default.disable_ipv6<span class=o>=</span><span class=m>1</span>
net.ipv6.conf.lo.disable_ipv6<span class=o>=</span><span class=m>1</span>
</code></pre></td></tr></table></div></div><p>同时还需要修改grub</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nv>GRUB_CMDLINE_LINUX</span><span class=o>=</span><span class=s2>&#34;crashkernel=auto ... ipv6.disable=1&#34;</span>
</code></pre></td></tr></table></div></div><p>更新grub：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>update-grub2
</code></pre></td></tr></table></div></div><h4 id=关闭密码认证>关闭密码认证</h4><p>出于安全考虑每个节点关闭掉密码认证使用密钥认证的方式。</p><p>在关掉之前我们需要在本地生成一个密钥，如果有的话直接将公钥放到节点<code>root</code>用户的<code>~/.ssh/authorized_keys</code>目录下即可。</p><p>如果没有密钥可以执行：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>ssh-keygen
</code></pre></td></tr></table></div></div><p>然后执行：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>ssh-copy-id root@hadoop-node1.nil.ml
ssh-copy-id root@hadoop-node2.nil.ml
ssh-copy-id root@hadoop-node3.nil.ml
</code></pre></td></tr></table></div></div><p>关闭ssh的密码认证：</p><p>修改<code>/etc/ssh/sshd_config</code>配置文件，修改以下内容：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-fallback data-lang=fallback>PasswordAuthentication no
ChallengeResponseAuthentication no
</code></pre></td></tr></table></div></div><p>保存并退出之后重启<code>sshd</code>服务</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>systemctl restart sshd
</code></pre></td></tr></table></div></div><h3 id=集群规划>集群规划</h3><p>节点以及角色的对应表格：</p><table><thead><tr><th>vm name</th><th>role</th></tr></thead><tbody><tr><td>ntp</td><td>ntp server</td></tr><tr><td>dns</td><td>dns server</td></tr><tr><td>kerberos-server</td><td>kerberos server</td></tr><tr><td>hadoop-node1.nil.ml</td><td>namenode-master, datanode, journalnode, nodemanager, zookeeper, DFSZKFailoverController</td></tr><tr><td>hadoop-node2.nil.ml</td><td>Namenode-Slave, DFSzkfailovercontroller, Resourcemanager-master, Datanode, JournalNode, NodeManager, Zookeeper</td></tr><tr><td>hadoop-node3.nil.ml</td><td>Resourcemanager-Slave, DataNode, JournalNode, Nodemanager, Zookeeper</td></tr></tbody></table><p>所用到用户如下表：</p><table><thead><tr><th>用户名</th><th>权限</th></tr></thead><tbody><tr><td><code>zookeeper</code></td><td>普通用户（启动zk）</td></tr><tr><td><code>dolphinscheduler</code></td><td>dolphinscheduler 管理用户</td></tr><tr><td><code>hdfs</code></td><td>HDFS 运行用户</td></tr><tr><td><code>hive</code></td><td>Hive 运行用户</td></tr></tbody></table><p>使用到的组：</p><table><thead><tr><th>组名</th><th>用户成员</th><th>说明</th></tr></thead><tbody><tr><td>hadoop</td><td>hdfs, hive, dolphinscheduler</td><td>大数据成员</td></tr></tbody></table><h3 id=端口说明>端口说明</h3><p>端口以及对应的服务如下表：</p><p>| 组件 | 服务 | 端口 | 配置 | 注解 |
|&mdash;&mdash;&mdash;&ndash;|&mdash;&mdash;|&mdash;&mdash;|
| | | |</p><h3 id=虚拟机网络>虚拟机网络</h3><p>虚拟机的网络分为：192.168.122.0/24</p><p>配置虚拟机的ip和主机名，关闭selinux和防火墙最后重启：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hostnamectl set-hostname hadoop-node1.nil.ml
nmcli con mod eth0 ipv4.method manual ipv4.addresses 192.168.122.40/24 ipv4.gateway 192.168.122.1 ipv4.dns <span class=s2>&#34;192.168.122.1, 114.114.114.114&#34;</span> connection.autoconnect yes
sed -i <span class=s1>&#39;s/SELINUX=enforcing/SELINUX=disabled/g&#39;</span> /etc/selinux/config
systemctl disable firewalld
reboot
</code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hostnamectl set-hostname hadoop-node2.nil.ml
nmcli con mod eth0 ipv4.method manual ipv4.addresses 192.168.122.41/24 ipv4.gateway 192.168.122.1 ipv4.dns <span class=s2>&#34;192.168.122.1, 114.114.114.114&#34;</span> connection.autoconnect yes
sed -i <span class=s1>&#39;s/SELINUX=enforcing/SELINUX=disabled/g&#39;</span> /etc/selinux/config
systemctl disable firewalld
reboot
</code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hostnamectl set-hostname hadoop-node3.nil.ml
nmcli con mod eth0 ipv4.method manual ipv4.addresses 192.168.122.42/24 ipv4.gateway 192.168.122.1 ipv4.dns <span class=s2>&#34;192.168.122.1, 114.114.114.114&#34;</span> connection.autoconnect yes
sed -i <span class=s1>&#39;s/SELINUX=enforcing/SELINUX=disabled/g&#39;</span> /etc/selinux/config
systemctl disable firewalld
reboot
</code></pre></td></tr></table></div></div><h3 id=添加管理用户>添加管理用户</h3><blockquote><p>创建<code>hadoop</code>组：</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo groupadd hadoop
</code></pre></td></tr></table></div></div><blockquote><p>sudoer 设置：</p></blockquote><p>编辑<code>/etc/sudoers</code>文件</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>%hadoop <span class=nv>ALL</span><span class=o>=(</span>ALL<span class=o>)</span> NOPASSWD: ALL
</code></pre></td></tr></table></div></div><blockquote><p>创建管理用户</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>useradd -m -G hadoop user -s /bin/bash
<span class=nb>echo</span> <span class=s2>&#34;user:changeme&#34;</span> <span class=p>|</span> sudo chpasswd
</code></pre></td></tr></table></div></div><blockquote><p>创建hdfs账户：</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo useradd -m hdfs  -s /bin/bash
<span class=nb>echo</span> <span class=s2>&#34;hdfs:changeme&#34;</span> <span class=p>|</span> sudo chpasswd
</code></pre></td></tr></table></div></div><blockquote><p>创建hive账户：</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo useradd -m hive  -s /bin/bash
<span class=nb>echo</span> <span class=s2>&#34;hive:changeme&#34;</span> <span class=p>|</span> sudo chpasswd
</code></pre></td></tr></table></div></div><blockquote><p>创建hbase账户：</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo useradd -m hbase  -s /bin/bash
<span class=nb>echo</span> <span class=s2>&#34;hbase:changeme&#34;</span> <span class=p>|</span> sudo chpasswd
</code></pre></td></tr></table></div></div><blockquote><p>创建spark用户：</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo useradd -m spark  -s /bin/bash
<span class=nb>echo</span> <span class=s2>&#34;spark:changeme&#34;</span> <span class=p>|</span> sudo chpasswd
</code></pre></td></tr></table></div></div><blockquote><p>创建dolphinscheduler账户：</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo useradd -m dolphinscheduler -G hadoop -s /bin/bash
<span class=nb>echo</span> <span class=s2>&#34;dolphinscheduler:changeme&#34;</span> <span class=p>|</span> sudo chpasswd
</code></pre></td></tr></table></div></div><p>添加<code>zookeeper</code>的用户：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo useradd -m zookeeper -s /bin/bash
<span class=nb>echo</span> <span class=s2>&#34;zookeeper:changeme&#34;</span> <span class=p>|</span> sudo chpasswd
</code></pre></td></tr></table></div></div><blockquote><p>锁定 root用户</p></blockquote><p>首先复制一下密钥，以免等下我们登录不进去系统。</p><p>切换到<code>user</code>用户</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>su - user
</code></pre></td></tr></table></div></div><p>创建文件夹：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>mkdir -pv ~/.ssh
</code></pre></td></tr></table></div></div><p>创建并编辑<code>~/.ssh/authorized_keys</code>文件，内容是你的公钥。</p><p>保存退出之后更改文件权限：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>chmod <span class=m>700</span> ~/.ssh
chmod <span class=m>600</span> ~/.ssh/authorized_keys
</code></pre></td></tr></table></div></div><p>验证是否可以正常连接到节点：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>ssh user@hadoop-node1.nil.ml
ssh user@hadoop-node1.nil.ml
ssh user@hadoop-node1.nil.ml
</code></pre></td></tr></table></div></div><p>没有问题之后锁定root账户：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo passwd -l root
</code></pre></td></tr></table></div></div><h3 id=创建上传包目录>创建上传包目录</h3><p>创建的目录以及对应的访问权限如下表：</p><table><thead><tr><th>目录</th><th>权限</th><th>备注</th></tr></thead><tbody><tr><td><code>/opt/packages</code></td><td>hadoop: g rwx</td><td>所有包存放的地方</td></tr></tbody></table><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo mkdir -pv /opt/packages
sudo chown -R user /opt/packages
</code></pre></td></tr></table></div></div><h3 id=ntp配置>NTP配置</h3><p>在集群里面节点的时间同步是非常重要的部分，这里来配置一下时间同步服务器。</p><p>首先调整一下时区：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo timedatectl set-timezone Asia/Shanghai
</code></pre></td></tr></table></div></div><p>在<code>hadoop-node1.nil.ml</code>节点上配置NTP服务器，首先安装<code>chrony</code>：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo yum install chrony -y
</code></pre></td></tr></table></div></div><p>修改<code>/etc/chrony.conf</code>配置文件，修改内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>server ntp.aliyun.com iburst
allow 192.168.122.0/24
</code></pre></td></tr></table></div></div><p>开机启动<code>chronyd</code>服务并启动：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo systemctl <span class=nb>enable</span> chronyd.service --now
</code></pre></td></tr></table></div></div><p>查看服务状态：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo systemctl status chronyd.service
</code></pre></td></tr></table></div></div><p>其他节点上也部署 chrony：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo yum install chrony -y
</code></pre></td></tr></table></div></div><p>修改<code>/etc/chrony.conf</code>配置文件：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo vi /etc/chrony.conf
</code></pre></td></tr></table></div></div><p>修改内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>server hadoop-node1.nil.ml iburst
</code></pre></td></tr></table></div></div><p>重启<code>chronyd</code>服务：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo systemctl restart chronyd.service
</code></pre></td></tr></table></div></div><p>查看同步状态：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>chronyc sources
</code></pre></td></tr></table></div></div><p>文件句柄：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>cat&gt;/etc/security/limits.d/hadoop.conf<span class=s>&lt;&lt;EOF
</span><span class=s>*       soft    nproc   131072
</span><span class=s>*       hard    nproc   131072
</span><span class=s>*       soft    nofile  131072
</span><span class=s>*       hard    nofile  131072
</span><span class=s>EOF</span>
</code></pre></td></tr></table></div></div><h3 id=安装jdk环境>安装JDK环境</h3><p>这里安装的是<a href=http://openjdk.java.net/>openjdk</a>8的版本：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo yum install -y  java-1.8.0-openjdk java-1.8.0-openjdk-devel java-1.8.0-openjdk-headless
</code></pre></td></tr></table></div></div><p>配置一下shell的变量，使得用户可以直接使用java。</p><p>编辑<code>/etc/profile</code>文件</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo vi /etc/profile
</code></pre></td></tr></table></div></div><blockquote><p>这个<code>JAVA_HOME</code>地址如何找可以直接查看<code>/usr/bin/java</code>指向那里然后看链接的位置得到</p></blockquote><p>在末尾添加：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>export</span> <span class=nv>JAVA_HOME</span><span class=o>=</span>/usr/lib/jvm/java-1.8.0-openjdk/
</code></pre></td></tr></table></div></div><p>保存退出之后，运行：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>source</span> /etc/profile <span class=o>&amp;&amp;</span> java -version
</code></pre></td></tr></table></div></div><p>如果成功输出以下内容就说明我们的Java环境已经配置成功了：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-fallback data-lang=fallback>openjdk version &#34;1.8.0_292&#34;
OpenJDK Runtime Environment (build 1.8.0_292-b10)
OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode)
</code></pre></td></tr></table></div></div><h3 id=上传包>上传包</h3><p>首先确保每个节点上已经安装了rsync：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo yum install rsync -y
</code></pre></td></tr></table></div></div><p>从本地的包上传到各个服务器：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>rsync -avz . -e ssh user@hadoop-node1.nil.ml:/opt/packages
rsync -avz . -e ssh user@hadoop-node2.nil.ml:/opt/packages
rsync -avz . -e ssh user@hadoop-node3.nil.ml:/opt/packages
</code></pre></td></tr></table></div></div><h2 id=安装和zookeeper>安装和zookeeper</h2><h3 id=安装zookeeper>安装zookeeper</h3><p>解压二进制包：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>cd</span> /opt/packages/
sudo tar -xf apache-zookeeper-3.6.3-bin.tar.gz
sudo mv apache-zookeeper-3.6.3-bin /opt/zookeeper
</code></pre></td></tr></table></div></div><p>创建数据目录</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo mkdir -pv /opt/zookeeper/data
</code></pre></td></tr></table></div></div><p>更改权限：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo chown -R zookeeper:zookeeper /opt/zookeeper
</code></pre></td></tr></table></div></div><h3 id=配置zookeeper>配置zookeeper</h3><p>切换到<code>zookeeper</code>用户：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo su - zookeeper
</code></pre></td></tr></table></div></div><p>创建并编辑配置文件<code>/opt/zookeeper/conf/zoo.cfg</code>：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>vi /opt/zookeeper/conf/zoo.cfg
</code></pre></td></tr></table></div></div><p>内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-fallback data-lang=fallback>tickTime=2000
initLimit=10
syncLimit=5
dataDir=/opt/zookeeper/data
clientPort=2181
server.1=hadoop-node1.nil.ml:2888:3888
server.2=hadoop-node2.nil.ml:2888:3888
server.3=hadoop-node3.nil.ml:2888:3888
</code></pre></td></tr></table></div></div><p>配置id：</p><blockquote><p>hadoop-node1.nil.ml</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>echo</span> <span class=s2>&#34;1&#34;</span> &gt; /opt/zookeeper/data/myid
</code></pre></td></tr></table></div></div><blockquote><p>hadoop-node2.nil.ml</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>echo</span> <span class=s2>&#34;2&#34;</span> &gt; /opt/zookeeper/data/myid
</code></pre></td></tr></table></div></div><blockquote><p>hadoop-node3.nil.ml</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>echo</span> <span class=s2>&#34;3&#34;</span> &gt; /opt/zookeeper/data/myid
</code></pre></td></tr></table></div></div><h3 id=启动zookeeper并检查状态>启动zookeeper并检查状态</h3><p>为了方便管理我们这里为zookeeper创建一个systemd的service。</p><p>创建并编辑<code>/lib/systemd/system/zookeeper.service</code></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo vi /lib/systemd/system/zookeeper.service
</code></pre></td></tr></table></div></div><p>内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=o>[</span>Unit<span class=o>]</span>
<span class=nv>Description</span><span class=o>=</span>Zookeeper Daemon
<span class=nv>Documentation</span><span class=o>=</span>http://zookeeper.apache.org
<span class=nv>Requires</span><span class=o>=</span>network.target
<span class=nv>After</span><span class=o>=</span>network.target

<span class=o>[</span>Service<span class=o>]</span>
<span class=nv>Type</span><span class=o>=</span>forking
<span class=nv>WorkingDirectory</span><span class=o>=</span>/opt/zookeeper
<span class=nv>User</span><span class=o>=</span>zookeeper
<span class=nv>Group</span><span class=o>=</span>zookeeper
<span class=nv>ExecStart</span><span class=o>=</span>/opt/zookeeper/bin/zkServer.sh start /opt/zookeeper/conf/zoo.cfg
<span class=nv>ExecStop</span><span class=o>=</span>/opt/zookeeper/bin/zkServer.sh stop /opt/zookeeper/conf/zoo.cfg
<span class=nv>ExecReload</span><span class=o>=</span>/opt/zookeeper/bin/zkServer.sh restart /opt/zookeeper/conf/zoo.cfg
<span class=nv>TimeoutSec</span><span class=o>=</span><span class=m>30</span>
<span class=nv>Restart</span><span class=o>=</span>on-failure

<span class=o>[</span>Install<span class=o>]</span>
<span class=nv>WantedBy</span><span class=o>=</span>default.target
</code></pre></td></tr></table></div></div><p>启动服务：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo systemctl start zookeeper
</code></pre></td></tr></table></div></div><p>查看服务状态：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo systemctl status zookeeper
</code></pre></td></tr></table></div></div><p>加入到开机启动：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo systemctl <span class=nb>enable</span> zookeeper
</code></pre></td></tr></table></div></div><h2 id=安装hadoop>安装hadoop</h2><h3 id=免密>免密</h3><blockquote><p>这里使用<code>hdfs</code>用户来做免密：</p></blockquote><p>首先切换到<code>hdfs</code>用户：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo su - hdfs
</code></pre></td></tr></table></div></div><p>生产密钥（三个节点都需要）：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>ssh-keygen
</code></pre></td></tr></table></div></div><p>这一步在<code>hadoop-node1</code>上操作：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>cat .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys
</code></pre></td></tr></table></div></div><p>这一步用于生成<code>.ssh/authorized_keys</code>文件。</p><p>接下来我们添加其他两个公钥</p><p>在其他的两个节点上查看生成的公钥然后添加到当前的<code>.ssh/authorized_keys</code>文件内：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>cat .ssh/id_rsa.pub
</code></pre></td></tr></table></div></div><p>将这个文件内容<code>.ssh/authorized_keys</code>复制到其他的两个点上。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>mkdir -pv ~/.ssh
</code></pre></td></tr></table></div></div><p>创建并编辑<code>~/.ssh/authorized_keys</code>内容就是从<code>hadoop-node1.nil.ml</code>复制过来的：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2tDyregbTpxwyPuTNwQy769G8gs+bd3CuRyneo3HomDHRZnx6vE14aLdHs8k1KK7ko3c3eKZ83zrytKbLv9Eq5zH22kmNG2Xp1fiXMGDex81SZ9qrPI2IXW6Dtk82w8nH8XGs+2BcA71RZWzXGBc+CJfUPnEyhKqsZpTFP8FZko/i8ptb9ShghY614etXNzKy9g0O0s9WD9rdBw/QoOC3xybD/aFfbZP+YFgPZSywT8ThXkdJhDucDS6WG9yvASxUAdXyPkjbdrBh0y/FzF3qKbvEunszWo7I27nXndQ8ew3uIp7p+rfjy6bDtgnyvhvSqXaKQ72umzIz/cvlWE17 hdfs@hadoop-node1.nil.ml
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC14y2KT1a4rBBKFLNH3vB1SVEOcV0nrVTFouMFIppADRMVqB2eEoB5p57jWn7vj+RrXPxFDP/qfoj6fO3M6KyHRg+mx0JJiT+LPNhb5M5tacPN79aDQ3Js/hZJHDWjerv42YdkuOkfozhIB8wAti7Pvc/C6/n2MjJly9PjH+mAC5WQl0QbLJmqTnS1yfmCFVhIYhTF9wS0GmTVHdspEcvXHKiNo0QpEmf1ezNPIcO4V5cMhbfdx2mstmx8OWQQdcZ7zBYDOOx5NpdIB5BxSp2yEDQjOfkEr8uc0QH2DhmTVCHwWS2tL+XYdkJPEuGoqQp4EZopeuFfM1gOZZIJsUTl hdfs@hadoop-node2.nil.ml
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDDqq8HXxd4c8/6dc371PkRWVhAyf239bW7w3FU3Qs3ZMcMWbu2UwqF5hlX/Wr55lH4VkvEkMywS4VZBRnr3mHiE+RpPyNdETpb9TAkAp+lHWpbkVVq+reUIpURKaqLZQiq535okktddNLOpZ0bKX4dAR4Iqs8H7OV+EFcSkoZo8BQzI4tuERlAmbJ6D2Ft4pjmE0ii4br0BjSQtAZcEjrc8JGAvus8sQ45UBVAARbk8zL0ekL67Yn7aA2Bws0UYDyX+iVCLrCBVqC0ftI7S39s6nP+50rjQX95ZTvtuPQk23JObN+FnW1acKmZbBkFgBMsoUTM95f3c8kmfpaX94O3 hdfs@hadoop-node3.nil.ml
</code></pre></td></tr></table></div></div><p>更改权限：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>chmod <span class=m>700</span> ~/.ssh
chmod <span class=m>600</span> ~/.ssh/authorized_keys
</code></pre></td></tr></table></div></div><p>验证一下是否成功：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>ssh hadoop-node2.nil.ml <span class=s2>&#34;hostname&#34;</span>
ssh hadoop-node3.nil.ml <span class=s2>&#34;hostname&#34;</span>
</code></pre></td></tr></table></div></div><h3 id=下载hadoop>下载hadoop</h3><p>这里下载我们需要的软件包</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>cd</span> /opt/packages
wget -c https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.2.2/hadoop-3.2.2.tar.gz
</code></pre></td></tr></table></div></div><p>解压，并放到单独的目录中：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo tar -xf hadoop-3.2.2.tar.gz
sudo ln -sf /opt/packages/hadoop-3.2.2 /opt/hadoop
</code></pre></td></tr></table></div></div><p>更改权限：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo chown -R hdfs:hdfs /opt/hadoop/
</code></pre></td></tr></table></div></div><p>在 <code>/etc/profile</code> 文件末尾追加：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>export</span> <span class=nv>HADOOP_HOME</span><span class=o>=</span>/opt/hadoop
<span class=nb>export</span> <span class=nv>HADOOP_INSTALL</span><span class=o>=</span><span class=nv>$HADOOP_HOME</span>
<span class=nb>export</span> <span class=nv>HADOOP_MAPRED_HOME</span><span class=o>=</span><span class=nv>$HADOOP_HOME</span>
<span class=nb>export</span> <span class=nv>HADOOP_COMMON_HOME</span><span class=o>=</span><span class=nv>$HADOOP_HOME</span>
<span class=nb>export</span> <span class=nv>HADOOP_HDFS_HOME</span><span class=o>=</span><span class=nv>$HADOOP_HOME</span>
<span class=nb>export</span> <span class=nv>HADOOP_YARN_HOME</span><span class=o>=</span><span class=nv>$HADOOP_HOME</span>
<span class=nb>export</span> <span class=nv>HADOOP_COMMON_LIB_NATIVE_DIR</span><span class=o>=</span><span class=nv>$HADOOP_HOME</span>/lib/native
<span class=nb>export</span> <span class=nv>PATH</span><span class=o>=</span><span class=nv>$PATH</span>:<span class=nv>$HADOOP_HOME</span>/sbin:<span class=nv>$HADOOP_HOME</span>/bin
<span class=nb>export</span> <span class=nv>HADOOP_OPTS</span><span class=o>=</span><span class=s2>&#34;-Djava.library.path=</span><span class=nv>$HADOOP_HOME</span><span class=s2>/lib/native&#34;</span>
</code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>source</span> /etc/profile
</code></pre></td></tr></table></div></div><p>创建hdfs的文件夹(每个节点)：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo mkdir -pv /opt/hdfs/<span class=o>{</span>namenode,datanode,journalnode<span class=o>}</span>
sudo chown -R hdfs:hadoop /opt/hdfs/
</code></pre></td></tr></table></div></div><blockquote><p>注意切换到hdfs用户：</p></blockquote><p>修改hadoop的配置</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>vi <span class=nv>$HADOOP_HOME</span>/etc/hadoop/hadoop-env.sh
</code></pre></td></tr></table></div></div><p>添加<code>JAVA_HOME</code>的配置：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>export</span> <span class=nv>JAVA_HOME</span><span class=o>=</span>/usr/lib/jvm/java-1.8.0-openjdk/
</code></pre></td></tr></table></div></div><p>创建hadoop的临时目录：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>mkdir -pv /opt/hadoop/tmp/
</code></pre></td></tr></table></div></div><p>在Master节点中，修改以下三个配置文件：</p><ul><li><a href=http://hadoop.apache.org/docs/r3.3.0/hadoop-project-dist/hadoop-common/core-default.xml><code>$HADOOP/etc/hadoop/core-site.xml</code></a></li><li><a href=http://hadoop.apache.org/docs/r3.1.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml><code>$HADOOP/etc/hadoop/hdfs-site.xml</code></a></li><li><a href=http://hadoop.apache.org/docs/r3.1.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml><code>$HADOOP/etc/hadoop/mapred-site.xml</code></a></li><li><a href=http://hadoop.apache.org/docs/r3.1.0/hadoop-yarn/hadoop-yarn-common/yarn-default.xml><code>$HADOOP/etc/yarn-site.xml</code></a></li><li><code>$HADOOP/etc/hadoop/workers</code></li></ul><p>修改<code>/opt/hadoop/etc/hadoop/core-site.xml</code>文件</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>vi /opt/hadoop/etc/hadoop/core-site.xml
</code></pre></td></tr></table></div></div><p>内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-xml data-lang=xml><span class=cp>&lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt;</span>
<span class=cp>&lt;?xml-stylesheet type=&#34;text/xsl&#34; href=&#34;configuration.xsl&#34;?&gt;</span>
<span class=nt>&lt;configuration&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>hadoop.tmp.dir<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>/opt/hadoop/tmp<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;final&gt;</span>true<span class=nt>&lt;/final&gt;</span>
    <span class=c>&lt;!-- base for other temporary directories --&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
    <span class=nt>&lt;property&gt;</span>
        <span class=nt>&lt;name&gt;</span>fs.defaultFS<span class=nt>&lt;/name&gt;</span>
        <span class=nt>&lt;value&gt;</span>hdfs://hadoop-cluster/<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>io.file.buffer.size<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>131072<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=c>&lt;!-- zk ha --&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>ha.zookeeper.quorum<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>hadoop-node1.nil.ml:2181,hadoop-node2.nil.ml:2181,hadoop-node3.nil.ml:2181<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>ha.zookeeper.session-timeout.ms<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>1000<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;description&gt;</span> connection zookeeper timeout ms<span class=nt>&lt;/description&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>ipc.client.connect.max.retries<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>1000<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;description&gt;</span>Indicates the number of retries a client will make to establish a server connection.<span class=nt>&lt;/description&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>ipc.client.connect.retry.interval<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>10000<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;description&gt;</span>Indicates the number of milliseconds a client will wait for before retrying to establish a server connection.<span class=nt>&lt;/description&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
     <span class=nt>&lt;name&gt;</span>hadoop.proxyuser.root.hosts<span class=nt>&lt;/name&gt;</span>
     <span class=nt>&lt;value&gt;</span>*<span class=nt>&lt;/value&gt;</span>
   <span class=nt>&lt;/property&gt;</span>
   <span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>hadoop.proxyuser.root.groups<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>*<span class=nt>&lt;/value&gt;</span>
   <span class=nt>&lt;/property&gt;</span>
<span class=nt>&lt;/configuration&gt;</span>
</code></pre></td></tr></table></div></div><p><code>hdfs-site.xml</code></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>vi /opt/hadoop/etc/hadoop/hdfs-site.xml
</code></pre></td></tr></table></div></div><p>hadoop-node1 节点：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>  1
</span><span class=lnt>  2
</span><span class=lnt>  3
</span><span class=lnt>  4
</span><span class=lnt>  5
</span><span class=lnt>  6
</span><span class=lnt>  7
</span><span class=lnt>  8
</span><span class=lnt>  9
</span><span class=lnt> 10
</span><span class=lnt> 11
</span><span class=lnt> 12
</span><span class=lnt> 13
</span><span class=lnt> 14
</span><span class=lnt> 15
</span><span class=lnt> 16
</span><span class=lnt> 17
</span><span class=lnt> 18
</span><span class=lnt> 19
</span><span class=lnt> 20
</span><span class=lnt> 21
</span><span class=lnt> 22
</span><span class=lnt> 23
</span><span class=lnt> 24
</span><span class=lnt> 25
</span><span class=lnt> 26
</span><span class=lnt> 27
</span><span class=lnt> 28
</span><span class=lnt> 29
</span><span class=lnt> 30
</span><span class=lnt> 31
</span><span class=lnt> 32
</span><span class=lnt> 33
</span><span class=lnt> 34
</span><span class=lnt> 35
</span><span class=lnt> 36
</span><span class=lnt> 37
</span><span class=lnt> 38
</span><span class=lnt> 39
</span><span class=lnt> 40
</span><span class=lnt> 41
</span><span class=lnt> 42
</span><span class=lnt> 43
</span><span class=lnt> 44
</span><span class=lnt> 45
</span><span class=lnt> 46
</span><span class=lnt> 47
</span><span class=lnt> 48
</span><span class=lnt> 49
</span><span class=lnt> 50
</span><span class=lnt> 51
</span><span class=lnt> 52
</span><span class=lnt> 53
</span><span class=lnt> 54
</span><span class=lnt> 55
</span><span class=lnt> 56
</span><span class=lnt> 57
</span><span class=lnt> 58
</span><span class=lnt> 59
</span><span class=lnt> 60
</span><span class=lnt> 61
</span><span class=lnt> 62
</span><span class=lnt> 63
</span><span class=lnt> 64
</span><span class=lnt> 65
</span><span class=lnt> 66
</span><span class=lnt> 67
</span><span class=lnt> 68
</span><span class=lnt> 69
</span><span class=lnt> 70
</span><span class=lnt> 71
</span><span class=lnt> 72
</span><span class=lnt> 73
</span><span class=lnt> 74
</span><span class=lnt> 75
</span><span class=lnt> 76
</span><span class=lnt> 77
</span><span class=lnt> 78
</span><span class=lnt> 79
</span><span class=lnt> 80
</span><span class=lnt> 81
</span><span class=lnt> 82
</span><span class=lnt> 83
</span><span class=lnt> 84
</span><span class=lnt> 85
</span><span class=lnt> 86
</span><span class=lnt> 87
</span><span class=lnt> 88
</span><span class=lnt> 89
</span><span class=lnt> 90
</span><span class=lnt> 91
</span><span class=lnt> 92
</span><span class=lnt> 93
</span><span class=lnt> 94
</span><span class=lnt> 95
</span><span class=lnt> 96
</span><span class=lnt> 97
</span><span class=lnt> 98
</span><span class=lnt> 99
</span><span class=lnt>100
</span><span class=lnt>101
</span><span class=lnt>102
</span><span class=lnt>103
</span><span class=lnt>104
</span><span class=lnt>105
</span><span class=lnt>106
</span><span class=lnt>107
</span><span class=lnt>108
</span><span class=lnt>109
</span><span class=lnt>110
</span><span class=lnt>111
</span><span class=lnt>112
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-xml data-lang=xml><span class=cp>&lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt;</span>
<span class=cp>&lt;?xml-stylesheet type=&#34;text/xsl&#34; href=&#34;configuration.xsl&#34;?&gt;</span>
<span class=nt>&lt;configuration&gt;</span>
	    <span class=c>&lt;!-- 存储的副本数量 --&gt;</span>
        <span class=nt>&lt;property&gt;</span>
                <span class=nt>&lt;name&gt;</span>dfs.replication<span class=nt>&lt;/name&gt;</span>
				<span class=c>&lt;!-- if 3 datanode this value should be 2 --&gt;</span>
                <span class=nt>&lt;value&gt;</span>2<span class=nt>&lt;/value&gt;</span>
        <span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- namenode 数据的存放位置 --&gt;</span>
        <span class=nt>&lt;property&gt;</span>
                <span class=nt>&lt;name&gt;</span>dfs.name.dir<span class=nt>&lt;/name&gt;</span>
                <span class=nt>&lt;value&gt;</span>/opt/hdfs/namenode<span class=nt>&lt;/value&gt;</span>
        <span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- datanode 数据的存放位置 --&gt;</span>
        <span class=nt>&lt;property&gt;</span>
                <span class=nt>&lt;name&gt;</span>dfs.data.dir<span class=nt>&lt;/name&gt;</span>
                <span class=nt>&lt;value&gt;</span>/opt/hdfs/datanode<span class=nt>&lt;/value&gt;</span>
        <span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- JournalNode在本地磁盘存放数据的位置 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.journalnode.edits.dir<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>/opt/hdfs/journalnode<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		
		<span class=c>&lt;!-- secondary 节点信息 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.namenode.secondary.http-address<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>hadoop-node2.nil.ml:9001<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- 集群名称 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.nameservices<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>hadoop-cluster<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- namenode节点名称 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.ha.namenodes.hadoop-cluster<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>nn1,nn2<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- nn1 的rpc通信地址 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.namenode.rpc-address.hadoop-cluster.nn1<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>hadoop-node1.nil.ml:9000<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- nn2 的 RPC 通信地址 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.namenode.rpc-address.hadoop-cluster.nn2<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>hadoop-node2.nil.ml:9000<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- nn1的http 通信地址 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.namenode.http-address.hadoop-cluster.nn1<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>hadoop-node1.nil.ml:50070<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- nn2 的 http 通信地址 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.namenode.http-address.hadoop-cluster.nn2<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>hadoop-node2.nil.ml:50070<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- hdfs web --&gt;</span>
	    <span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.webhdfs.enabled<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>true<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- 指定 NameNode的edits元数据的共享存储位置也就是JournalNode列表url的配置 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.namenode.shared.edits.dir<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>qjournal://hadoop-node1.nil.ml:8485;hadoop-node2.nil.ml:8485;hadoop-node3.nil.ml:8485/hadoop-cluster<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- 开启NameNode失败自动切换 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.ha.automatic-failover.enabled<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>true<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- 指定失败之后的自动切换实现模式 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.client.failover.proxy.provider.hadoop-cluster<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- 配置隔离机制方法，多个机制使用换行分割：每个机制占用一行 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.ha.fencing.methods<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>
				sshfence
				shell(/bin/true)
			<span class=nt>&lt;/value&gt;</span>	
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- 使用 sshfence隔离机制的时候需要配置ssh免密登录 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>/home/hdfs/.ssh/id_rsa<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- 配置sshfence隔离机制超时时间 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>30000<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>ha.failover-controller.cli-check.rpc-timeout.ms<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>60000<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- block access token --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
            <span class=nt>&lt;name&gt;</span>dfs.block.access.token.enable<span class=nt>&lt;/name&gt;</span>
            <span class=nt>&lt;value&gt;</span>true<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=nt>&lt;property&gt;</span>
            <span class=nt>&lt;name&gt;</span>dfs.webhdfs.enabled<span class=nt>&lt;/name&gt;</span>
            <span class=nt>&lt;value&gt;</span>true<span class=nt>&lt;/value&gt;</span>
        <span class=nt>&lt;/property&gt;</span>
<span class=nt>&lt;/configuration&gt;</span>
</code></pre></td></tr></table></div></div><p>连接地址线程数计算：</p><p>这里写了一个python的小程序：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>math</span>

<span class=n>num</span><span class=o>=</span><span class=nb>int</span><span class=p>(</span><span class=nb>input</span> <span class=p>(</span><span class=s2>&#34;Enter the cluster node count: &#34;</span><span class=p>))</span>
<span class=n>th</span><span class=o>=</span><span class=p>(</span><span class=n>math</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>num</span><span class=p>)</span> <span class=o>*</span> <span class=mi>20</span><span class=p>)</span>
<span class=k>print</span> <span class=p>(</span><span class=s2>&#34;The dfs namnode handler count is &#34;</span><span class=p>,</span> <span class=nb>int</span><span class=p>(</span><span class=n>th</span><span class=p>))</span>
</code></pre></td></tr></table></div></div><p>关于这个参数的连接：</p><p><a href=https://blog.csdn.net/qq_43081842/article/details/102672420>https://blog.csdn.net/qq_43081842/article/details/102672420</a></p><p><code>fs.defaultFS：</code>NameNode地址
<code>hadoop.tmp.dir：</code>Hadoop临时目录</p><p><code>dfs.namenode.name.dir</code>：保存FSImage的目录，存放NameNode的<code>metadata</code>
<code>dfs.datanode.data.dir</code>：保存HDFS数据的目录，存放DataNode的多个数据块
<code>dfs.replication</code>：HDFS存储的临时备份数量，有两个Worker节点，因此数值为2</p><p>编辑<code>mapred-site.xml</code>配置文件：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>vi /opt/hadoop/etc/hadoop/mapred-site.xml
</code></pre></td></tr></table></div></div><p>内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-xml data-lang=xml><span class=cp>&lt;?xml version=&#34;1.0&#34;?&gt;</span>
<span class=cp>&lt;?xml-stylesheet type=&#34;text/xsl&#34; href=&#34;configuration.xsl&#34;?&gt;</span>
<span class=nt>&lt;configuration&gt;</span>
	    <span class=c>&lt;!-- 指定MR使用框架为yarn --&gt;</span>
        <span class=nt>&lt;property&gt;</span>
                <span class=nt>&lt;name&gt;</span>mapreduce.framework.name<span class=nt>&lt;/name&gt;</span>
                <span class=nt>&lt;value&gt;</span>yarn<span class=nt>&lt;/value&gt;</span>
        <span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- 指定 MR Jobhistory的地址 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>mapreduce.jobhistory.address<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>hadoop-node1.nil.ml:10020<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- 任务历史服务器的web地址 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>mapreduce.jobhistory.webapp.address<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>hadoop-node1.nil.ml:19888<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
<span class=nt>&lt;/configuration&gt;</span>
</code></pre></td></tr></table></div></div><p>修改yarn配置文件<code>/opt/hadoop/etc/hadoop/yarn-site.xml </code>：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>vi <span class=nv>$HADOOP_HOME</span>/etc/hadoop/yarn-site.xml
</code></pre></td></tr></table></div></div><p>内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span><span class=lnt>78
</span><span class=lnt>79
</span><span class=lnt>80
</span><span class=lnt>81
</span><span class=lnt>82
</span><span class=lnt>83
</span><span class=lnt>84
</span><span class=lnt>85
</span><span class=lnt>86
</span><span class=lnt>87
</span><span class=lnt>88
</span><span class=lnt>89
</span><span class=lnt>90
</span><span class=lnt>91
</span><span class=lnt>92
</span><span class=lnt>93
</span><span class=lnt>94
</span><span class=lnt>95
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-xml data-lang=xml><span class=cp>&lt;?xml version=&#34;1.0&#34;?&gt;</span>
<span class=nt>&lt;configuration&gt;</span> 
	<span class=c>&lt;!-- 开启MR高可用 --&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>yarn.resourcemanager.ha.enabled<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>true<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=c>&lt;!-- 指定RM的cluster id --&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span> yarn.resourcemanager.cluster-id<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>yrc<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=c>&lt;!-- 指定 RM的名字 --&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>yarn.resourcemanager.ha.rm-ids<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>rm1,rm2<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=c>&lt;!-- 分别指定RM的地址 --&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>yarn.resourcemanager.hostname.rm1<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>hadoop-node1.nil.ml<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>yarn.resourcemanager.webapp.address.rm1<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>hadoop-node1.nil.ml:8088<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>yarn.resourcemanager.webapp.address.rm2<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>hadoop-node2.nil.ml:8088<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>	
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>yarn.resourcemanager.hostname.rm2<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>hadoop-node2.nil.ml<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>	
	
	<span class=c>&lt;!-- 指定 zk集群的地址 --&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>hadoop.zk.address<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>hadoop-node1.nil.ml:2181,hadoop-node2.nil.ml:2181,hadoop-node3.nil.ml:2181<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
    <span class=c>&lt;!-- 指定 MR 走 shuffle --&gt;</span> 
	<span class=nt>&lt;property&gt;</span> 
		<span class=nt>&lt;name&gt;</span>yarn.nodemanager.aux-services<span class=nt>&lt;/name&gt;</span> 
		<span class=nt>&lt;value&gt;</span>mapreduce_shuffle<span class=nt>&lt;/value&gt;</span> 
	<span class=nt>&lt;/property&gt;</span> 
     <span class=nt>&lt;property&gt;</span>
         <span class=nt>&lt;name&gt;</span>yarn.log-aggregation-enable<span class=nt>&lt;/name&gt;</span>
         <span class=nt>&lt;value&gt;</span>true<span class=nt>&lt;/value&gt;</span>
     <span class=nt>&lt;/property&gt;</span>
     <span class=nt>&lt;property&gt;</span>
         <span class=nt>&lt;name&gt;</span>yarn.log-aggregation.retain-seconds<span class=nt>&lt;/name&gt;</span>
         <span class=nt>&lt;value&gt;</span>86400<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;/property&gt;</span>
	<span class=c>&lt;!-- 启用自动恢复 --&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>yarn.resourcemanager.recovery.enabled<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>true<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=c>&lt;!-- 指定 resourcemanager的状态信息存储在 zookeeper集群上 --&gt;</span>
	 <span class=nt>&lt;property&gt;</span>
         <span class=nt>&lt;name&gt;</span>yarn.resourcemanager.store.class<span class=nt>&lt;/name&gt;</span>
         <span class=nt>&lt;value&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class=nt>&lt;/value&gt;</span>
     <span class=nt>&lt;/property&gt;</span>
    <span class=nt>&lt;property&gt;</span>
        <span class=nt>&lt;name&gt;</span>yarn.log-aggregation-enable<span class=nt>&lt;/name&gt;</span>
        <span class=nt>&lt;value&gt;</span>true<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;/property&gt;</span>
    <span class=c>&lt;!-- 日志保留时间设置7天 --&gt;</span>
    <span class=nt>&lt;property&gt;</span>
        <span class=nt>&lt;name&gt;</span>yarn.log-aggregation.retain-seconds<span class=nt>&lt;/name&gt;</span>
        <span class=nt>&lt;value&gt;</span>604800<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;/property&gt;</span>
    <span class=nt>&lt;property&gt;</span>
        <span class=nt>&lt;description&gt;</span>Indicate to clients whether Timeline service is enabled or not.
        If enabled, the TimelineClient library used by end-users will post entities
        and events to the Timeline server.<span class=nt>&lt;/description&gt;</span>
        <span class=nt>&lt;name&gt;</span>yarn.timeline-service.enabled<span class=nt>&lt;/name&gt;</span>
        <span class=nt>&lt;value&gt;</span>true<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;/property&gt;</span>

    <span class=nt>&lt;property&gt;</span>
        <span class=nt>&lt;description&gt;</span>The setting that controls whether yarn system metrics is
        published on the timeline server or not by RM.<span class=nt>&lt;/description&gt;</span>
        <span class=nt>&lt;name&gt;</span>yarn.resourcemanager.system-metrics-publisher.enabled<span class=nt>&lt;/name&gt;</span>
        <span class=nt>&lt;value&gt;</span>true<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;/property&gt;</span>

    <span class=nt>&lt;property&gt;</span>
        <span class=nt>&lt;description&gt;</span>Indicate to clients whether to query generic application
        data from timeline history-service or not. If not enabled then application
        data is queried only from Resource Manager.<span class=nt>&lt;/description&gt;</span>
        <span class=nt>&lt;name&gt;</span>yarn.timeline-service.generic-application-history.enabled<span class=nt>&lt;/name&gt;</span>
        <span class=nt>&lt;value&gt;</span>true<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;/property&gt;</span>
<span class=nt>&lt;/configuration&gt;</span>
</code></pre></td></tr></table></div></div><p>配置woker节点：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>vi /opt/hadoop/etc/hadoop/workers
</code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hadoop-node1.nil.ml
hadoop-node2.nil.ml
hadoop-node3.nil.ml
</code></pre></td></tr></table></div></div><p>复制文件到各个节点：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>rsync -avz /opt/hadoop -e ssh hadoop-node2.nil.ml:/opt/
rsync -avz /opt/hadoop -e ssh hadoop-node3.nil.ml:/opt/
</code></pre></td></tr></table></div></div><h3 id=启动>启动</h3><p>首先启动 journalnode（所有节点）</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hdfs --daemon start journalnode
</code></pre></td></tr></table></div></div><p>初始化namenode (hadoop-node1)节点：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hdfs namenode -format 
</code></pre></td></tr></table></div></div><p>复制<code>namenode</code>文件到hadoop-node2节点上：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>cd</span> /opt/hdfs/namenode 
scp -r current hadoop-node2.nil.ml:<span class=nv>$PWD</span>
</code></pre></td></tr></table></div></div><p>格式化 zkfc</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hdfs zkfc -formatZK
</code></pre></td></tr></table></div></div><p>启动<code>hdfs</code></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>start-dfs.sh
</code></pre></td></tr></table></div></div><p>启动yarn</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>start-yarn.sh
</code></pre></td></tr></table></div></div><p>查看yarn状态：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>yarn rmadmin -getServiceState rm1
</code></pre></td></tr></table></div></div><p>查看HDFS HA节点状态：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hdfs haadmin -getServiceState nn1
</code></pre></td></tr></table></div></div><p>如果发yarn状态不对需要手动启动resourcemanager的时候可以执行：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>yarn-daemons.sh start resourcemanager
</code></pre></td></tr></table></div></div><p>启动MR的任务历史服务器</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>mr-jobhistory-daemon.sh start historyserver
</code></pre></td></tr></table></div></div><blockquote><p>Web访问地址：</p></blockquote><blockquote><blockquote><p>HDFS：</p></blockquote></blockquote><ul><li>hadoop-node1.nil.ml:50070</li><li>hadoop-node2.nil.ml:50070</li></ul><blockquote><blockquote><p>yarn</p></blockquote></blockquote><ul><li>hadoop-node1.nil.ml:8088</li><li>hadoop-node2.nil.ml:8088</li></ul><blockquote><blockquote><p>jobhistory</p></blockquote></blockquote><ul><li>hadoop-node1.nil.ml:19888</li></ul><h3 id=hadoop-服务-systemd-service-文件>hadoop 服务 systemd service 文件</h3><p>为了方便管理这里去创建几个systemd file用于去管理服务</p><p>hdfs</p><p>创建并编辑<code>/lib/systemd/system/hadoop-dfs.service</code></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo vi /lib/systemd/system/hadoop-dfs.service
</code></pre></td></tr></table></div></div><p>内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=o>[</span>Unit<span class=o>]</span>
<span class=nv>Description</span><span class=o>=</span>Hadoop DFS namenode and datanode
<span class=nv>After</span><span class=o>=</span>syslog.target network.target remote-fs.target nss-lookup.target network-online.target
<span class=nv>Requires</span><span class=o>=</span>network-online.target

<span class=o>[</span>Service<span class=o>]</span>
<span class=nv>User</span><span class=o>=</span>hdfs
<span class=nv>Group</span><span class=o>=</span>hdfs
<span class=nv>Type</span><span class=o>=</span>forking
<span class=nv>ExecStart</span><span class=o>=</span>/opt/hadoop/sbin/start-dfs.sh
<span class=nv>ExecStop</span><span class=o>=</span>/opt/hadoop/sbin/stop-dfs.sh
<span class=nv>WorkingDirectory</span><span class=o>=</span>/opt/hadoop/
<span class=nv>Environment</span><span class=o>=</span><span class=nv>JAVA_HOME</span><span class=o>=</span>/usr/lib/jvm/java-1.8.0-openjdk/
<span class=nv>Environment</span><span class=o>=</span><span class=nv>HADOOP_HOME</span><span class=o>=</span>/opt/hadoop/
<span class=nv>TimeoutStartSec</span><span class=o>=</span>2min
<span class=nv>Restart</span><span class=o>=</span>on-failure
<span class=nv>PIDFile</span><span class=o>=</span>/tmp/hadoop-hadoop-namenode.pid

<span class=o>[</span>Install<span class=o>]</span>
<span class=nv>WantedBy</span><span class=o>=</span>multi-user.target
</code></pre></td></tr></table></div></div><p>启动：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo systemctl start hadoop-dfs.service
</code></pre></td></tr></table></div></div><p>查看状态：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo systemctl status hadoop-dfs.service
</code></pre></td></tr></table></div></div><p>开机启动：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo systemctl <span class=nb>enable</span> hadoop-dfs.service
</code></pre></td></tr></table></div></div><p>yarn</p><p>创建并编辑<code>/lib/systemd/system/hadoop-yarn.service</code></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo vi /lib/systemd/system/hadoop-yarn.service
</code></pre></td></tr></table></div></div><p>内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=o>[</span>Unit<span class=o>]</span>
<span class=nv>Description</span><span class=o>=</span>Hadoop Yarn
<span class=nv>After</span><span class=o>=</span>syslog.target network.target remote-fs.target nss-lookup.target network-online.target
<span class=nv>Requires</span><span class=o>=</span>network-online.target

<span class=o>[</span>Service<span class=o>]</span>
<span class=nv>User</span><span class=o>=</span>hdfs
<span class=nv>Group</span><span class=o>=</span>hdfs
<span class=nv>Type</span><span class=o>=</span>forking
<span class=nv>ExecStart</span><span class=o>=</span>/opt/hadoop/sbin/start-yarn.sh
<span class=nv>ExecStop</span><span class=o>=</span>/opt/hadoop/sbin/stop-yarn.sh
<span class=nv>WorkingDirectory</span><span class=o>=</span>/opt/hadoop/
<span class=nv>Environment</span><span class=o>=</span><span class=nv>JAVA_HOME</span><span class=o>=</span>/usr/lib/jvm/java-1.8.0-openjdk/
<span class=nv>Environment</span><span class=o>=</span><span class=nv>HADOOP_HOME</span><span class=o>=</span>/opt/hadoop/
<span class=nv>TimeoutStartSec</span><span class=o>=</span>2min
<span class=nv>Restart</span><span class=o>=</span>on-failure
<span class=nv>PIDFile</span><span class=o>=</span>/tmp/hadoop-hadoop-namenode.pid

<span class=o>[</span>Install<span class=o>]</span>
<span class=nv>WantedBy</span><span class=o>=</span>multi-user.target
</code></pre></td></tr></table></div></div><p>启动：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>systemctl start hadoop-yarn
</code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>systemctl <span class=nb>enable</span> hadoop-yarn
</code></pre></td></tr></table></div></div><p>jobhistory</p><h3 id=hdfs-测试>HDFS 测试</h3><p>在hdfs上创建文件夹：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hdfs dfs -mkdir /test1
hdfs dfs -mkdir /logs 
</code></pre></td></tr></table></div></div><p>查看：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hdfs dfs -ls / 
</code></pre></td></tr></table></div></div><p>把系统的<code>/var/log/</code>下面的所有内容丢在我们创建的<code>/logs/</code>文件夹下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hdfs dfs -put /var/log/* /logs/
</code></pre></td></tr></table></div></div><h3 id=ha验证>HA验证</h3><p>在这个章节我们将会测试之前部署hdfs HA是否可以正常使用</p><h4 id=干掉active-namenode>干掉active namenode</h4><p>首先我们干掉active namenode然后看看集群有什么变化</p><h3 id=为每个服务编写systemd-services-文件>为每个服务编写systemd services 文件</h3><h2 id=hive>hive</h2><p>解压hive包：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>
<span class=nb>cd</span> /opt/packatges
sudo tar -xf apache-hive-3.1.2-bin.tar.gz
sudo ln -sf  /opt/packages/apache-hive-3.1.2-bin /opt/hive
sudo chown -R hive:hive /opt/hive/
</code></pre></td></tr></table></div></div><p>修改<code>/etc/profile</code>文件：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>export</span> <span class=nv>HIVE_HOME</span><span class=o>=</span>/opt/hive
<span class=nb>export</span> <span class=nv>PATH</span><span class=o>=</span><span class=nv>$HIVE_HOME</span>/bin:<span class=nv>$PATH</span>
<span class=nb>export</span> <span class=nv>HIVE_CONF_DIR</span><span class=o>=</span><span class=nv>$HIVE_HOME</span>/conf
<span class=nb>export</span> <span class=nv>HIVE_AUX_JARS_PATH</span><span class=o>=</span><span class=nv>$HIVE_HOME</span>/lib
</code></pre></td></tr></table></div></div><p>配置数据库集成：</p><p>创建并编辑<code>/etc/yum.repos.d/mysql-57-ce.repo</code>文件，内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=o>[</span>mysql-57<span class=o>]</span>
<span class=nv>name</span> <span class=o>=</span> mysql-57
<span class=nv>baseurl</span> <span class=o>=</span> https://mirror.tuna.tsinghua.edu.cn/mysql/yum/mysql57-community-el7/
<span class=nb>enable</span> <span class=o>=</span> <span class=m>1</span>
<span class=nv>gpgcheck</span> <span class=o>=</span> <span class=m>0</span>
</code></pre></td></tr></table></div></div><p>更新本地的缓存：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>yum makecache
</code></pre></td></tr></table></div></div><blockquote><p>安装Mysql</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>yum install -y mysql-community-client mysql-community-server
</code></pre></td></tr></table></div></div><blockquote><p>配置Mysql</p></blockquote><p>启动Mysql服务，然后找到随机生成的那个临时密码，再去修改它：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>systemctl start mysqld 
grep <span class=s2>&#34;password&#34;</span> /var/log/mysqld.log
2021-03-23T13:33:22.126793Z <span class=m>1</span> <span class=o>[</span>Note<span class=o>]</span> A temporary password is generated <span class=k>for</span> root@localhost: iNJhEzzSw2%j
</code></pre></td></tr></table></div></div><p>这里这个<code>iNJhEzzSw2%j</code>临时的Mysql密码，我们用这个密码登陆到当前的mysql中：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>mysql -uroot -piNJhEzzSw2%j
</code></pre></td></tr></table></div></div><p>设置新的root密码：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-sql data-lang=sql><span class=k>set</span> <span class=k>global</span> <span class=n>validate_password_policy</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span>
<span class=k>set</span> <span class=k>global</span> <span class=n>validate_password_length</span><span class=o>=</span><span class=mi>6</span><span class=p>;</span>
<span class=k>ALTER</span> <span class=k>USER</span> <span class=s1>&#39;root&#39;</span><span class=o>@</span><span class=s1>&#39;localhost&#39;</span> <span class=n>IDENTIFIED</span> <span class=k>BY</span> <span class=s1>&#39;123456&#39;</span><span class=p>;</span>
</code></pre></td></tr></table></div></div><p>创建hive数据库以及对应的账户：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-sql data-lang=sql><span class=k>CREATE</span> <span class=k>DATABASE</span> <span class=n>hive</span> <span class=k>DEFAULT</span> <span class=nb>CHARACTER</span> <span class=k>SET</span> <span class=n>utf8</span> <span class=k>DEFAULT</span> <span class=k>COLLATE</span> <span class=n>utf8_general_ci</span><span class=p>;</span>
<span class=k>CREATE</span> <span class=k>USER</span> <span class=s1>&#39;hive&#39;</span><span class=o>@</span><span class=s1>&#39;hadoop-node1.nil.ml&#39;</span> <span class=n>IDENTIFIED</span> <span class=k>BY</span> <span class=s1>&#39;123456&#39;</span><span class=p>;</span>
<span class=k>GRANT</span> <span class=k>ALL</span> <span class=k>ON</span> <span class=n>hive</span><span class=p>.</span><span class=o>*</span> <span class=k>TO</span> <span class=s1>&#39;hive&#39;</span><span class=o>@</span><span class=s1>&#39;hadoop-node1.nil.ml&#39;</span><span class=p>;</span>
<span class=n>FLUSH</span> <span class=k>PRIVILEGES</span><span class=p>;</span>
</code></pre></td></tr></table></div></div><p>将mysqld服务重新启动并加入开机启动项：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>systemctl restart mysqld
systemctl <span class=nb>enable</span> mysqld
systemctl status mysqld
</code></pre></td></tr></table></div></div><blockquote><p>注意切换到hive用户</p></blockquote><p>配置<code>hive-env.sh</code>文件：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>cp -v  /opt/hive/conf/hive-env.sh.template /opt/hive/conf/hive-env.sh
vi /opt/hive/conf/hive-env.sh
</code></pre></td></tr></table></div></div><p>内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-fallback data-lang=fallback>export HADOOP_HOME=/opt/hadoop
export HIVE_HOME=/opt/hive
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HIVE_AUX_JARS_PATH=$HIVE_HOME/lib
export HIVE_CONF_DIR=$HIVE_HOME/conf
</code></pre></td></tr></table></div></div><p>修改配置文件：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>vi /opt/hive/conf/hive-site.xml
</code></pre></td></tr></table></div></div><p>内容如下</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>  1
</span><span class=lnt>  2
</span><span class=lnt>  3
</span><span class=lnt>  4
</span><span class=lnt>  5
</span><span class=lnt>  6
</span><span class=lnt>  7
</span><span class=lnt>  8
</span><span class=lnt>  9
</span><span class=lnt> 10
</span><span class=lnt> 11
</span><span class=lnt> 12
</span><span class=lnt> 13
</span><span class=lnt> 14
</span><span class=lnt> 15
</span><span class=lnt> 16
</span><span class=lnt> 17
</span><span class=lnt> 18
</span><span class=lnt> 19
</span><span class=lnt> 20
</span><span class=lnt> 21
</span><span class=lnt> 22
</span><span class=lnt> 23
</span><span class=lnt> 24
</span><span class=lnt> 25
</span><span class=lnt> 26
</span><span class=lnt> 27
</span><span class=lnt> 28
</span><span class=lnt> 29
</span><span class=lnt> 30
</span><span class=lnt> 31
</span><span class=lnt> 32
</span><span class=lnt> 33
</span><span class=lnt> 34
</span><span class=lnt> 35
</span><span class=lnt> 36
</span><span class=lnt> 37
</span><span class=lnt> 38
</span><span class=lnt> 39
</span><span class=lnt> 40
</span><span class=lnt> 41
</span><span class=lnt> 42
</span><span class=lnt> 43
</span><span class=lnt> 44
</span><span class=lnt> 45
</span><span class=lnt> 46
</span><span class=lnt> 47
</span><span class=lnt> 48
</span><span class=lnt> 49
</span><span class=lnt> 50
</span><span class=lnt> 51
</span><span class=lnt> 52
</span><span class=lnt> 53
</span><span class=lnt> 54
</span><span class=lnt> 55
</span><span class=lnt> 56
</span><span class=lnt> 57
</span><span class=lnt> 58
</span><span class=lnt> 59
</span><span class=lnt> 60
</span><span class=lnt> 61
</span><span class=lnt> 62
</span><span class=lnt> 63
</span><span class=lnt> 64
</span><span class=lnt> 65
</span><span class=lnt> 66
</span><span class=lnt> 67
</span><span class=lnt> 68
</span><span class=lnt> 69
</span><span class=lnt> 70
</span><span class=lnt> 71
</span><span class=lnt> 72
</span><span class=lnt> 73
</span><span class=lnt> 74
</span><span class=lnt> 75
</span><span class=lnt> 76
</span><span class=lnt> 77
</span><span class=lnt> 78
</span><span class=lnt> 79
</span><span class=lnt> 80
</span><span class=lnt> 81
</span><span class=lnt> 82
</span><span class=lnt> 83
</span><span class=lnt> 84
</span><span class=lnt> 85
</span><span class=lnt> 86
</span><span class=lnt> 87
</span><span class=lnt> 88
</span><span class=lnt> 89
</span><span class=lnt> 90
</span><span class=lnt> 91
</span><span class=lnt> 92
</span><span class=lnt> 93
</span><span class=lnt> 94
</span><span class=lnt> 95
</span><span class=lnt> 96
</span><span class=lnt> 97
</span><span class=lnt> 98
</span><span class=lnt> 99
</span><span class=lnt>100
</span><span class=lnt>101
</span><span class=lnt>102
</span><span class=lnt>103
</span><span class=lnt>104
</span><span class=lnt>105
</span><span class=lnt>106
</span><span class=lnt>107
</span><span class=lnt>108
</span><span class=lnt>109
</span><span class=lnt>110
</span><span class=lnt>111
</span><span class=lnt>112
</span><span class=lnt>113
</span><span class=lnt>114
</span><span class=lnt>115
</span><span class=lnt>116
</span><span class=lnt>117
</span><span class=lnt>118
</span><span class=lnt>119
</span><span class=lnt>120
</span><span class=lnt>121
</span><span class=lnt>122
</span><span class=lnt>123
</span><span class=lnt>124
</span><span class=lnt>125
</span><span class=lnt>126
</span><span class=lnt>127
</span><span class=lnt>128
</span><span class=lnt>129
</span><span class=lnt>130
</span><span class=lnt>131
</span><span class=lnt>132
</span><span class=lnt>133
</span><span class=lnt>134
</span><span class=lnt>135
</span><span class=lnt>136
</span><span class=lnt>137
</span><span class=lnt>138
</span><span class=lnt>139
</span><span class=lnt>140
</span><span class=lnt>141
</span><span class=lnt>142
</span><span class=lnt>143
</span><span class=lnt>144
</span><span class=lnt>145
</span><span class=lnt>146
</span><span class=lnt>147
</span><span class=lnt>148
</span><span class=lnt>149
</span><span class=lnt>150
</span><span class=lnt>151
</span><span class=lnt>152
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-xml data-lang=xml><span class=cp>&lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34; standalone=&#34;no&#34;?&gt;</span>
<span class=cp>&lt;?xml-stylesheet type=&#34;text/xsl&#34; href=&#34;configuration.xsl&#34;?&gt;</span>
<span class=nt>&lt;configuration&gt;</span>
<span class=c>&lt;!-- 数据库配置 --&gt;</span>
 <span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>javax.jdo.option.ConnectionURL<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>jdbc:mysql://hadoop-node1.nil.ml:3306/hive?createDatabaseIfNotExist=true<span class=ni>&amp;amp;</span>useUnicode=true<span class=ni>&amp;amp;</span>characterEncoding=UTF-8<span class=ni>&amp;amp;</span>useSSL=false<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;description&gt;</span>JDBC connect string for a JDBC metastore<span class=nt>&lt;/description&gt;</span>
<span class=nt>&lt;/property&gt;</span>
<span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>javax.jdo.option.ConnectionDriverName<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>com.mysql.jdbc.Driver<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;description&gt;</span>Driver class name for a JDBC metastore<span class=nt>&lt;/description&gt;</span>
<span class=nt>&lt;/property&gt;</span>
<span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>javax.jdo.option.ConnectionUserName<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>hive<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;description&gt;</span>username to use against metastore database<span class=nt>&lt;/description&gt;</span>
<span class=nt>&lt;/property&gt;</span>
<span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>javax.jdo.option.ConnectionPassword<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>123456<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;description&gt;</span>password to use against metastore database<span class=nt>&lt;/description&gt;</span>
<span class=nt>&lt;/property&gt;</span>

<span class=c>&lt;!-- hive的工作目录 --&gt;</span>

 <span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>hive.exec.local.scratchdir<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>/opt/hive/tmp/hiveuser<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;description&gt;</span>Local scratch space for Hive jobs<span class=nt>&lt;/description&gt;</span>
  <span class=nt>&lt;/property&gt;</span>
  <span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>hive.downloaded.resources.dir<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>/opt/hive/tmp/${hive.session.id}_resources<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;description&gt;</span>Temporary local directory for added resources in the remote file system.<span class=nt>&lt;/description&gt;</span>
  <span class=nt>&lt;/property&gt;</span>
 <span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>hive.querylog.location<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>/opt/hive/tmp/qrylog<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;description&gt;</span>Location of Hive run time structured log file<span class=nt>&lt;/description&gt;</span>
  <span class=nt>&lt;/property&gt;</span>

<span class=c>&lt;!-- 元数据 --&gt;</span>
<span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>datanucleus.schema.autoCreateAll<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>true<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;description&gt;</span>Auto creates necessary schema on a startup if one doesn&#39;t exist. Set this to false, after creating it once.To enable auto create also set hive.metastore.schema.verification=false. Auto creation is not recommended for production use cases, run schematool command instead.<span class=nt>&lt;/description&gt;</span>
  <span class=nt>&lt;/property&gt;</span>
  <span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>hive.metastore.schema.verification<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>false<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;description&gt;</span>
      Enforce metastore schema version consistency.
      True: Verify that version information stored in is compatible with one from Hive jars.  Also disable automatic
            schema migration attempt. Users are required to manually migrate schema after Hive upgrade which ensures
            proper metastore schema migration. (Default)
      False: Warn if the version information stored in metastore doesn&#39;t match with one from in Hive jars.
    <span class=nt>&lt;/description&gt;</span>
  <span class=nt>&lt;/property&gt;</span>

<span class=c>&lt;!-- zookeeper --&gt;</span>


 <span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>hive.zookeeper.quorum<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>
       hadoop-node1.nil.ml:2181,hadoop-node2.nil.ml:2181,hadoop-node3.nil.ml:2181
    <span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;description&gt;</span>
      List of ZooKeeper servers to talk to. This is needed for:
      1. Read/write locks - when hive.lock.manager is set to
      org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager,
      1. When HiveServer2 supports service discovery via Zookeeper.
      2. For delegation token storage if zookeeper store is used, if
      hive.cluster.delegation.token.store.zookeeper.connectString is not set
      1. LLAP daemon registry service
      2. Leader selection for privilege synchronizer
    <span class=nt>&lt;/description&gt;</span>
  <span class=nt>&lt;/property&gt;</span>

 <span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>hive.server2.support.dynamic.service.discovery<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>true<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;description&gt;</span>Whether HiveServer2 supports dynamic service discovery for its clients. To support this, each instance of HiveServer2 currently uses ZooKeeper to register itself, when it is brought up. JDBC/ODBC clients should use the ZooKeeper ensemble: hive.zookeeper.quorum in their connection string.<span class=nt>&lt;/description&gt;</span>
  <span class=nt>&lt;/property&gt;</span>
  <span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>hive.server2.zookeeper.namespace<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>hiveserver2_zk<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;description&gt;</span>The parent node in ZooKeeper used by HiveServer2 when supporting dynamic service discovery.<span class=nt>&lt;/description&gt;</span>
  <span class=nt>&lt;/property&gt;</span>
  <span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>hive.server2.zookeeper.publish.configs<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>true<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;description&gt;</span>Whether we should publish HiveServer2&#39;s configs to ZooKeeper.<span class=nt>&lt;/description&gt;</span>
  <span class=nt>&lt;/property&gt;</span>
<span class=c>&lt;!-- 日志 --&gt;</span>
<span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>hive.server2.logging.operation.log.location<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>/opt/hive/tmp/operation_logs<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;description&gt;</span>Top level directory where operation logs are stored if logging functionality is enabled<span class=nt>&lt;/description&gt;</span>
  <span class=nt>&lt;/property&gt;</span>

 <span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>hive.server2.thrift.client.user<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>root<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;description&gt;</span>Username to use against thrift client<span class=nt>&lt;/description&gt;</span>
  <span class=nt>&lt;/property&gt;</span>
  <span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>hive.server2.thrift.client.password<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>changeme<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;description&gt;</span>Password to use against thrift client<span class=nt>&lt;/description&gt;</span>
  <span class=nt>&lt;/property&gt;</span>

<span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>hive.server2.thrift.port<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>10000<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;description&gt;</span>Port number of HiveServer2 Thrift interface when hive.server2.transport.mode is &#39;binary&#39;.<span class=nt>&lt;/description&gt;</span>
  <span class=nt>&lt;/property&gt;</span>
<span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>hive.server2.transport.mode<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>binary<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;description&gt;</span>
      Expects one of [binary, http].
      Transport mode of HiveServer2.
    <span class=nt>&lt;/description&gt;</span>
  <span class=nt>&lt;/property&gt;</span>
  <span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>hive.server2.thrift.bind.host<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>hadoop-node1.nil.ml<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;description&gt;</span>Bind host on which to run the HiveServer2 Thrift service.<span class=nt>&lt;/description&gt;</span>
  <span class=nt>&lt;/property&gt;</span>
<span class=c>&lt;!-- hdfs --&gt;</span>
<span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>hive.exec.scratchdir<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>/tmp/hive<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;description&gt;</span>HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: ${hive.exec.scratchdir}/<span class=ni>&amp;lt;</span>username<span class=ni>&amp;gt;</span> is created, with ${hive.scratch.dir.permission}.<span class=nt>&lt;/description&gt;</span>
  <span class=nt>&lt;/property&gt;</span>
  <span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>hive.repl.rootdir<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>/user/hive/repl/<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;description&gt;</span>HDFS root dir for all replication dumps.<span class=nt>&lt;/description&gt;</span>
  <span class=nt>&lt;/property&gt;</span>
<span class=c>&lt;!-- 连接的doas --&gt;</span>
<span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>hive.server2.enable.doAs<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>FALSE<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;description&gt;</span>Setting this property to true will have HiveServer2 execute Hive operations as the user making the calls to it.
    <span class=nt>&lt;/description&gt;</span>
<span class=nt>&lt;/property&gt;</span>

<span class=nt>&lt;/configuration&gt;</span>
</code></pre></td></tr></table></div></div><p>确保hadoop的<code>core-site.xml</code>配置文件中有以下内容:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-xml data-lang=xml><span class=nt>&lt;property&gt;</span>
     <span class=nt>&lt;name&gt;</span>hadoop.proxyuser.root.hosts<span class=nt>&lt;/name&gt;</span>
     <span class=nt>&lt;value&gt;</span>*<span class=nt>&lt;/value&gt;</span>
   <span class=nt>&lt;/property&gt;</span>
   <span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>hadoop.proxyuser.root.groups<span class=nt>&lt;/name&gt;</span>
    <span class=nt>&lt;value&gt;</span>*<span class=nt>&lt;/value&gt;</span>
<span class=nt>&lt;/property&gt;</span>
</code></pre></td></tr></table></div></div><p>创建日志目录：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>mkdir -pv /opt/hive/tmp/hiveuser /opt/hive/tmp/qrylog /opt/hive/tmp/operation_logs
</code></pre></td></tr></table></div></div><blockquote><p>切换到<code>hdfs</code>用户</p></blockquote><p>创建hive在hdfs上的目录：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hadoop fs -mkdir  -p  /user/hive/repl /user/hive/warehouse /tmp/hive
hadoop fs -chown -R hive /user/hive/repl /user/hive/warehouse
hadoop fs -chmod -R <span class=m>777</span> /tmp  /user/hive
</code></pre></td></tr></table></div></div><p>安装jdbc：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>yum install -y mysql-connector-java
</code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>cp -v /opt/hive/conf/hive-exec-log4j2.properties.template /opt/hive/conf/hive-exec-log4j2.properties
cp -v /opt/hive/conf/hive-log4j2.properties.template /opt/hive/conf/hive-log4j2.properties
rm -f /opt/hive/lib/guava-19.0.jar
cp -v /opt/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar /opt/hive/lib/
cp -v /usr/share/java/mysql-connector-java.jar /opt/hive/lib/
</code></pre></td></tr></table></div></div><p>初始化：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nv>$HIVE_HOME</span>/bin/schematool -dbType mysql -initSchema
</code></pre></td></tr></table></div></div><p>创建并编辑<code>/opt/hive/bin/daemon.sh</code>：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>vi /opt/hive/bin/daemon.sh
</code></pre></td></tr></table></div></div><p>内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=cp>#!/bin/sh
</span><span class=cp></span>. /etc/profile
<span class=k>if</span> <span class=o>[</span> <span class=nv>$#</span> -ne <span class=m>2</span> <span class=o>]</span> <span class=p>;</span><span class=k>then</span>
	<span class=nb>echo</span> <span class=s2>&#34;please input two params,first is (metastore|hiveserver2),second is (start|stop)&#34;</span>
	<span class=nb>exit</span> <span class=m>0</span>
<span class=k>fi</span>
<span class=k>if</span> <span class=o>[</span> <span class=s2>&#34;</span><span class=nv>$1</span><span class=s2>&#34;</span> <span class=o>==</span> <span class=s2>&#34;metastore&#34;</span> <span class=o>]</span> <span class=p>;</span> <span class=k>then</span>
	<span class=k>if</span> <span class=o>[</span> <span class=s2>&#34;</span><span class=nv>$2</span><span class=s2>&#34;</span> <span class=o>==</span> <span class=s2>&#34;start&#34;</span> <span class=o>]</span> <span class=p>;</span> <span class=k>then</span>
		<span class=nb>echo</span> <span class=s2>&#34;now is start metastore&#34;</span>
		nohup <span class=nv>$HIVE_HOME</span>/bin/hive --service metastore &gt; /var/log/hive/hive-metastore.log 2&gt;<span class=p>&amp;</span><span class=m>1</span>  <span class=p>&amp;</span>
		<span class=nb>exit</span> <span class=m>0</span>
	<span class=k>elif</span> <span class=o>[</span> <span class=s2>&#34;</span><span class=nv>$2</span><span class=s2>&#34;</span> <span class=o>==</span> <span class=s2>&#34;stop&#34;</span> <span class=o>]</span> <span class=p>;</span> <span class=k>then</span>
		ps -ef <span class=p>|</span>grep <span class=o>[</span>h<span class=o>]</span>ive-metastore <span class=p>|</span>awk <span class=s1>&#39;{print $2}&#39;</span> <span class=p>|</span> xargs <span class=nb>kill</span>
		<span class=nb>echo</span> <span class=s2>&#34;-------metastore has stop&#34;</span>
		<span class=nb>exit</span> <span class=m>0</span>
	<span class=k>else</span>
		<span class=nb>echo</span> <span class=s2>&#34;second param please input &#39;start&#39; or &#39;stop&#39;&#34;</span>
		<span class=nb>exit</span> <span class=m>0</span>
	<span class=k>fi</span>
<span class=k>elif</span> <span class=o>[</span> <span class=s2>&#34;</span><span class=nv>$1</span><span class=s2>&#34;</span> <span class=o>==</span> <span class=s2>&#34;hiveserver2&#34;</span> <span class=o>]</span> <span class=p>;</span> <span class=k>then</span>
	<span class=k>if</span> <span class=o>[</span> <span class=s2>&#34;</span><span class=nv>$2</span><span class=s2>&#34;</span> <span class=o>==</span> <span class=s2>&#34;start&#34;</span> <span class=o>]</span> <span class=p>;</span> <span class=k>then</span>

        	<span class=nb>echo</span> <span class=s2>&#34;now is start hiveserver2&#34;</span>
		nohup <span class=nv>$HIVE_HOME</span>/bin/hive --service hiveserver2 &gt; /var/log/hive/hiveserver2.log 2&gt;<span class=p>&amp;</span><span class=m>1</span>  <span class=p>&amp;</span>
		<span class=nb>exit</span> <span class=m>0</span>
        <span class=k>elif</span> <span class=o>[</span> <span class=s2>&#34;</span><span class=nv>$2</span><span class=s2>&#34;</span> <span class=o>==</span> <span class=s2>&#34;stop&#34;</span> <span class=o>]</span> <span class=p>;</span> <span class=k>then</span>
		ps -ef <span class=p>|</span>grep <span class=o>[</span>h<span class=o>]</span>iveserver <span class=p>|</span>awk <span class=s1>&#39;{print $2}&#39;</span> <span class=p>|</span> xargs <span class=nb>kill</span>
        <span class=k>else</span>
                <span class=nb>echo</span> <span class=s2>&#34;second param please input &#39;start&#39; or &#39;stop&#39;&#34;</span>
		<span class=nb>exit</span> <span class=m>0</span>
        <span class=k>fi</span>
<span class=k>else</span>
	<span class=nb>echo</span> <span class=s2>&#34;first param please input &#39;metastore&#39; or &#39;hiveserver2&#39;&#34;</span>
<span class=k>fi</span>
</code></pre></td></tr></table></div></div><p>添加执行权限：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>chmod +x /opt/hive/bin/daemon.sh
</code></pre></td></tr></table></div></div><p>创建services文件：</p><blockquote><p><code>hive-hiveserver2.service</code>
创建并编辑：<code>/lib/systemd/system/hive-hiveserver2.service</code>文件</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>vi /lib/systemd/system/hive-hiveserver2.service
</code></pre></td></tr></table></div></div><p>内容如下</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-fallback data-lang=fallback>[Unit]
Description=metastore
Wants=network-online.target
After=network-online.target

[Service]
Type=forking 
User=hive
Group=hadoop
Environment=&#34;HADOOP_HOME=/opt/hadoop&#34; 
ExecStart=/opt/hive/bin/daemon.sh hiveserver2 start
ExecStop=/opt/hive/bin/daemon.sh hiveserver2 stop
Restart=no

[Install]
WantedBy=multi-user.target
</code></pre></td></tr></table></div></div><p><code>hive-metastore.service</code>
创建并编辑<code>/lib/systemd/system/hive-metastore.service</code>：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>vi /lib/systemd/system/hive-metastore.service
</code></pre></td></tr></table></div></div><p>内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-fallback data-lang=fallback>[Unit]
Description=metastore
Wants=network-online.target
After=network-online.target

[Service]
Type=forking 
User=hive
Group=hadoop
Environment=&#34;HADOOP_HOME=/opt/hadoop&#34;
ExecStart=/opt/hive/bin/daemon.sh metastore start
ExecStop=/opt/hive/bin/daemon.sh metastore stop
Restart=no

[Install]
WantedBy=multi-user.target
</code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>systemctl daemon-reload
</code></pre></td></tr></table></div></div><p>创建hive日志的目录：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>mkdir -pv /var/log/hive
chown -R hive:hadoop /var/log/hive/
</code></pre></td></tr></table></div></div><p>先启动元数据，后启动hiveserver2</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>systemctl start hive-metastore.service
systemctl status hive-metastore.service
systemctl start hive-hiveserver2.service
systemctl status hive-hiveserver2.service
</code></pre></td></tr></table></div></div><p>运行hive：</p><p>验证：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nv>$HIVE_HOME</span>/bin/beeline -u jdbc:hive2://hadoop-node1.nil.ml:10000  -n root --color<span class=o>=</span><span class=nb>true</span> --silent<span class=o>=</span><span class=nb>false</span>
</code></pre></td></tr></table></div></div><p>创建一个数据库试试看：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-sql data-lang=sql><span class=k>create</span> <span class=k>database</span> <span class=n>test</span><span class=p>;</span>
</code></pre></td></tr></table></div></div><h2 id=hbase>hbase</h2><p>解压：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>cd</span> /opt/packages/
tar -xf hbase-2.3.5-bin.tar.gz
mv hbase-2.3.5 /opt/hbase
sudo chown -R hbase:hbase /opt/hbase
</code></pre></td></tr></table></div></div><p>修改环境变量<code>/etc/profile</code></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>export</span> <span class=nv>HBASE_HOME</span><span class=o>=</span>/opt/hbase
<span class=nb>export</span> <span class=nv>PATH</span><span class=o>=</span><span class=nv>$HBASE_HOME</span>/bin:<span class=nv>$PATH</span>
</code></pre></td></tr></table></div></div><p>切换到<code>hbase</code>用户</p><p>vi /opt/hbase/conf/hbase-env.sh</p><p>export HBASE_HOME=/opt/hbase
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk/
export HBASE_CLASSPATH=$HBASE_HOME/conf
export HADOOP_HOME=/opt/hadoop
export HBASE_LOG_DIR=$HBASE_HOME/logs</p><p>vi /opt/hbase/conf/hbase-site.xml</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-xml data-lang=xml><span class=cp>&lt;?xml version=&#34;1.0&#34;?&gt;</span>
<span class=nt>&lt;configuration&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>hbase.rootdir<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>hdfs://hadoop-cluster/hbase<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>hbase.cluster.distributed<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>true<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>hbase.master<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>hadoop-node1.nil.ml:60000<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>hbase.zookeeper.quorum<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>hadoop-node1.nil.ml,hadoop-node2.nil.ml,hadoop-node3.nil.ml<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
<span class=nt>&lt;/configuration&gt;</span>
</code></pre></td></tr></table></div></div><p>regionservers</p><p>vi /opt/hbase/conf/regionservers</p><p>hadoop-node1.nil.ml
hadoop-node2.nil.ml
hadoop-node3.nil.ml</p><p>创建日志目录：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>mkdir -pv /opt/hbase/logs
</code></pre></td></tr></table></div></div><p>复制hbase到从节点中</p><p>scp -r /opt/hbase hadoop-node2.nil.ml:/opt
scp -r /opt/hbase hadoop-node3.nil.ml:/opt</p><p>更改权限：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>chown -R hbase:hbase /opt/hbase
</code></pre></td></tr></table></div></div><p>在hdfs上创建hbase的目录以及授权：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hadoop fs -mkdir /hbase
hadoop fs -chown hbase /hbase
</code></pre></td></tr></table></div></div><p>配置ssh免密</p><p>生成密钥：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>ssh-keygen
</code></pre></td></tr></table></div></div><p>创建文件夹：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>mkdir -pv ~/.ssh
</code></pre></td></tr></table></div></div><p>创建并编辑<code>~/.ssh/authorized_keys</code>文件，内容是你的公钥。</p><p>保存退出之后更改文件权限：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>chmod <span class=m>700</span> ~/.ssh
chmod <span class=m>600</span> ~/.ssh/authorized_keys
</code></pre></td></tr></table></div></div><p>验证是否可以正常连接到节点：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>ssh hbase@hadoop-node1.nil.ml <span class=s2>&#34;whoami&#34;</span>
ssh hbase@hadoop-node1.nil.ml <span class=s2>&#34;whoami&#34;</span>
ssh hbase@hadoop-node1.nil.ml <span class=s2>&#34;whoami&#34;</span>


启动：
<span class=sb>```</span>shell
/opt/hbase/bin/start-hbase.sh
</code></pre></td></tr></table></div></div><p>验证：</p><h2 id=spark>Spark</h2><p>hadoop缺点:
1.表达能力有限(MapReduce)
2.磁盘IO开销大(shuffle)
3.延迟高
spark:
1.Spark的计算模式属于MapReduce,在借鉴Hadoop MapReduce优点的同时很好地解决了MapReduce所面临的问题
2.不局限于Map和Reduce操作，还提供了多种数据集操作类型，编程模型比Hadoop MapReduce更灵活
3.Spark提供了内存计算，可将中间结果放到内存中，对于迭代运算效率更高
4.Spark基于DAG的任务调度执行机制，要优于Hadoop MapReduce的迭代执行机制(函数调用)</p><pre><code>使用Hadoop进行迭代计算非常耗资源；
Spark将数据载入内存后，之后的迭代计算都可以直接使用内存中的中间结果作运算，避免了从磁盘中频繁读取数据
</code></pre><p>Spark包括 Master、Slaves、Driver和每个worker上负责任务执行进程Executor</p><p>Master是集群的管理者 Cluster Manager 支持 Standalone Yarn， Mesos</p><p>配置构建环境：</p><ul><li>Maven 3.6.3</li><li>Scala 2.12</li><li>Java 8</li><li>shasum</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>mkdir -pv tools
<span class=nb>cd</span> tools
wget -c https://ftp.wayne.edu/apache/maven/maven-3/3.8.1/binaries/apache-maven-3.8.1-bin.tar.gz
</code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>wget -c https://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-3.1.2/spark-3.1.2.tgz
</code></pre></td></tr></table></div></div><p>去掉hive jar包的支持</p><p>构建一个可以运行的发型版：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>export</span> <span class=nv>MAVEN_OPTS</span><span class=o>=</span><span class=s2>&#34;-Xmx2g -XX:ReservedCodeCacheSize=1g&#34;</span>
<span class=nb>export</span> <span class=nv>PATH</span><span class=o>=</span><span class=nv>$PATH</span>
./dev/make-distribution.sh --name <span class=s2>&#34;hadoop3-without-hive&#34;</span> --tgz <span class=s2>&#34;-Pyarn,hadoop-provided,hadoop-3.2&#34;</span>
</code></pre></td></tr></table></div></div><p>spark用户免密</p><p>切换到<code>spark</code>用户：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>su - spark
</code></pre></td></tr></table></div></div><p>生产密钥（三个节点都需要）：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>ssh-keygen
</code></pre></td></tr></table></div></div><p>这一步在<code>hadoop-node1</code>上操作：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>cat .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys
</code></pre></td></tr></table></div></div><p>这一步用于生成<code>.ssh/authorized_keys</code>文件。</p><p>接下来我们添加其他两个公钥</p><p>在其他的两个节点上查看生成的公钥然后添加到当前的<code>.ssh/authorized_keys</code>文件内：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>cat .ssh/id_rsa.pub
</code></pre></td></tr></table></div></div><p>将这个文件内容<code>.ssh/authorized_keys</code>复制到其他的两个点上。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>mkdir -pv ~/.ssh
</code></pre></td></tr></table></div></div><p>创建并编辑<code>~/.ssh/authorized_keys</code>内容就是从<code>hadoop-node1.nil.ml</code>复制过来的：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2tDyregbTpxwyPuTNwQy769G8gs+bd3CuRyneo3HomDHRZnx6vE14aLdHs8k1KK7ko3c3eKZ83zrytKbLv9Eq5zH22kmNG2Xp1fiXMGDex81SZ9qrPI2IXW6Dtk82w8nH8XGs+2BcA71RZWzXGBc+CJfUPnEyhKqsZpTFP8FZko/i8ptb9ShghY614etXNzKy9g0O0s9WD9rdBw/QoOC3xybD/aFfbZP+YFgPZSywT8ThXkdJhDucDS6WG9yvASxUAdXyPkjbdrBh0y/FzF3qKbvEunszWo7I27nXndQ8ew3uIp7p+rfjy6bDtgnyvhvSqXaKQ72umzIz/cvlWE17 spark@hadoop-node1.nil.ml
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC14y2KT1a4rBBKFLNH3vB1SVEOcV0nrVTFouMFIppADRMVqB2eEoB5p57jWn7vj+RrXPxFDP/qfoj6fO3M6KyHRg+mx0JJiT+LPNhb5M5tacPN79aDQ3Js/hZJHDWjerv42YdkuOkfozhIB8wAti7Pvc/C6/n2MjJly9PjH+mAC5WQl0QbLJmqTnS1yfmCFVhIYhTF9wS0GmTVHdspEcvXHKiNo0QpEmf1ezNPIcO4V5cMhbfdx2mstmx8OWQQdcZ7zBYDOOx5NpdIB5BxSp2yEDQjOfkEr8uc0QH2DhmTVCHwWS2tL+XYdkJPEuGoqQp4EZopeuFfM1gOZZIJsUTl spark@hadoop-node2.nil.ml
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDDqq8HXxd4c8/6dc371PkRWVhAyf239bW7w3FU3Qs3ZMcMWbu2UwqF5hlX/Wr55lH4VkvEkMywS4VZBRnr3mHiE+RpPyNdETpb9TAkAp+lHWpbkVVq+reUIpURKaqLZQiq535okktddNLOpZ0bKX4dAR4Iqs8H7OV+EFcSkoZo8BQzI4tuERlAmbJ6D2Ft4pjmE0ii4br0BjSQtAZcEjrc8JGAvus8sQ45UBVAARbk8zL0ekL67Yn7aA2Bws0UYDyX+iVCLrCBVqC0ftI7S39s6nP+50rjQX95ZTvtuPQk23JObN+FnW1acKmZbBkFgBMsoUTM95f3c8kmfpaX94O3 spark@hadoop-node3.nil.ml
</code></pre></td></tr></table></div></div><p>更改权限：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>chmod <span class=m>700</span> ~/.ssh
chmod <span class=m>600</span> ~/.ssh/authorized_keys
</code></pre></td></tr></table></div></div><p>验证一下是否成功：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>ssh hadoop-node2.nil.ml <span class=s2>&#34;hostname&#34;</span>
ssh hadoop-node3.nil.ml <span class=s2>&#34;hostname&#34;</span>
</code></pre></td></tr></table></div></div><p>部署spark</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>cd</span> /opt/packages
tar -xf spark-3.1.2-bin-hadoop3.2.tgz
ln -sf /opt/packages/spark-3.1.2-bin-hadoop3.2 /opt/spark
chown -R spark:spark /opt/spark/
</code></pre></td></tr></table></div></div><p>在<code>/etc/profile</code>文件中添加如下内容：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>export</span> <span class=nv>SPARK_HOME</span><span class=o>=</span>/opt/spark
<span class=nb>export</span> <span class=nv>PATH</span><span class=o>=</span><span class=nv>$PATH</span>:<span class=nv>$SPARK_HOME</span>/bin
</code></pre></td></tr></table></div></div><p>修改<code>spark-env.sh</code>文件：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>cp -v /opt/spark/conf/spark-env.sh.template /opt/spark/conf/spark-env.sh
vi /opt/spark/conf/spark-env.sh
</code></pre></td></tr></table></div></div><p>添加内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>export</span> <span class=nv>JAVA_HOME</span><span class=o>=</span>/usr/lib/jvm/java-1.8.0-openjdk/
<span class=nb>export</span> <span class=nv>SPARK_MASTER_HOST</span><span class=o>=</span>hadoop-node1.nil.ml
</code></pre></td></tr></table></div></div><p>修改<code>workers</code>文件：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>cp -v /opt/spark/conf/workers.template /opt/spark/conf/workers
vi /opt/spark/conf/workers
</code></pre></td></tr></table></div></div><p>内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hadoop-node1.nil.ml
hadoop-node2.nil.ml
hadoop-node3.nil.ml
</code></pre></td></tr></table></div></div><p>同步配置文件：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>rsync -avz /opt/spark/ -e ssh hadoop-node2.nil.ml:/opt/spark/
rsync -avz /opt/spark/ -e ssh hadoop-node3.nil.ml:/opt/spark/
</code></pre></td></tr></table></div></div><p>启动服务：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sbin/start-master.sh
</code></pre></td></tr></table></div></div><p>MasterUI</p><p>hadoop-node1.nil.ml:8081</p><p>启动salves</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>/opt/spark/sbin/start-workers.sh
</code></pre></td></tr></table></div></div><p>测试：</p><p>安装python3（每个<code>worker</code>节点）</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>yum install python3-devel python3 python3-wheel -y
</code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>spark-submit --master spark://hadoop-node1.nil.ml:7077 /opt/spark/examples/src/main/python/pi.py
</code></pre></td></tr></table></div></div><p>配置spark on hive</p><p>修改<code>hive-site.xml</code>添加如下内容：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-xml data-lang=xml><span class=c>&lt;!-- hive on spark--&gt;</span>
  <span class=nt>&lt;property&gt;</span>
    <span class=nt>&lt;name&gt;</span>hive.excution.engine<span class=nt>&lt;/name&gt;</span>
	<span class=nt>&lt;value&gt;</span>spark<span class=nt>&lt;/value&gt;</span>
  <span class=nt>&lt;/property&gt;</span>
</code></pre></td></tr></table></div></div><p>复制spark的jar包到hive的lib下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>cp -v /opt/spark/jars/scala-library-2.12.10.jar /opt/hive/lib
cp -v /opt/spark/jars/spark-core_2.12-3.1.2.jar /opt/hive/lib
cp -v /opt/spark/jars/spark-network-common_2.12-3.1.2.jar /opt/hive/lib
</code></pre></td></tr></table></div></div><h2 id=海豚调度>海豚调度</h2><h3 id=安装和配置海豚调度>安装和配置海豚调度</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>tar -xf apache-dolphinscheduler-1.3.6-bin.tar.gz
</code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo mv apache-dolphinscheduler-1.3.6-bin /opt/dolphinscheduler-bin
</code></pre></td></tr></table></div></div><p>创建用户：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo useradd -m -g sudo dolphinscheduler -s /bin/bash
<span class=nb>echo</span> <span class=s2>&#34;dolphinscheduler:changeme&#34;</span> <span class=p>|</span> sudo chpasswd
</code></pre></td></tr></table></div></div><p>修改权限：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo chown -R dolphinscheduler /opt/dolphinscheduler-bin
</code></pre></td></tr></table></div></div><p>切换用户：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo su - dolphinscheduler
</code></pre></td></tr></table></div></div><p>配置ssh免密：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>su dolphinscheduler<span class=p>;</span>

ssh-keygen -t rsa -P <span class=s1>&#39;&#39;</span> -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
chmod <span class=m>600</span> ~/.ssh/authorized_keys
</code></pre></td></tr></table></div></div><p>更改默认的数据库密码规则：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-sql data-lang=sql><span class=k>show</span> <span class=n>VARIABLES</span> <span class=k>like</span> <span class=s2>&#34;%password%&#34;</span><span class=p>;</span>
</code></pre></td></tr></table></div></div><p>这里可以看到输出的<code>validate_password.policy</code>字段的值为<code>MEDIUM</code>中等的密码强度，根据需要更改为不同的值。</p><p>这里修改为最低的密码等级（仅用于测试）。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-sql data-lang=sql><span class=k>SET</span> <span class=n>PERSIST</span> <span class=n>valicate_password</span><span class=p>.</span><span class=n>policy</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</code></pre></td></tr></table></div></div><p>数据库初始化：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-sql data-lang=sql><span class=k>CREATE</span> <span class=k>DATABASE</span> <span class=n>dolphinscheduler</span> <span class=k>DEFAULT</span> <span class=nb>CHARACTER</span> <span class=k>SET</span> <span class=n>utf8</span> <span class=k>DEFAULT</span> <span class=k>COLLATE</span> <span class=n>utf8_general_ci</span><span class=p>;</span>
<span class=k>CREATE</span> <span class=k>USER</span> <span class=s1>&#39;dolphinscheduler&#39;</span><span class=o>@</span><span class=s1>&#39;localhost&#39;</span> <span class=n>IDENTIFIED</span> <span class=k>BY</span> <span class=s1>&#39;changeme&#39;</span><span class=p>;</span>
<span class=n>ES</span> <span class=k>ON</span> <span class=n>dolphinscheduler</span><span class=p>.</span><span class=o>*</span> <span class=k>TO</span> <span class=s1>&#39;dolphinscheduler&#39;</span><span class=o>@</span><span class=s1>&#39;localhost&#39;</span><span class=p>;</span>
<span class=n>flush</span> <span class=k>privileges</span><span class=p>;</span>
</code></pre></td></tr></table></div></div><p>创建表和导入基础数据</p><p>首先安装mysql的连接器：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>wget -c https://cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-8.0.26-1.el7.noarch.rpm
</code></pre></td></tr></table></div></div><p>安装：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo yum install -y mysql-connector-java-8.0.26-1.el7.noarch.rpm
</code></pre></td></tr></table></div></div><p>复制库文件到<code>dolphinscheduler</code>的<code>lib</code>目录下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>cp -v /usr/share/java/mysql-connector-java-8.0.24.jar /opt/dolphinscheduler-bin/lib/
</code></pre></td></tr></table></div></div><p>修改<code>conf</code>目录下的<code>datasource.properties</code>中的配置：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>vi conf/datasource.properties
</code></pre></td></tr></table></div></div><p>注释掉 postgresql的部分</p><p>取消注释掉 mysql部分的，内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>spring.datasource.driver-class-name<span class=o>=</span>com.mysql.jdbc.Driver
spring.datasource.url<span class=o>=</span>jdbc:mysql://localhost:3306/dolphinscheduler?useUnicode<span class=o>=</span>true<span class=p>&amp;</span><span class=nv>characterEncoding</span><span class=o>=</span>UTF-8<span class=p>&amp;</span><span class=nv>allowMultiQueries</span><span class=o>=</span><span class=nb>true</span>
spring.datasource.username<span class=o>=</span>dolphinscheduler
spring.datasource.password<span class=o>=</span>changeme
</code></pre></td></tr></table></div></div><p>执行<code>script</code>目录下的数据库脚本：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sh script/create-dolphinscheduler.sh
</code></pre></td></tr></table></div></div><p>修改运行参数：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>vi conf/env/dolphinscheduler_env.sh
</code></pre></td></tr></table></div></div><p>内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>export</span> <span class=nv>HADOOP_HOME</span><span class=o>=</span>/opt/hadoop
<span class=nb>export</span> <span class=nv>HADOOP_CONF_DIR</span><span class=o>=</span>/opt/hadoop/etc/hadoop
<span class=nb>export</span> <span class=nv>JAVA_HOME</span><span class=o>=</span>/usr/lib/jvm/java-1.8.0-openjdk/
<span class=nb>export</span> <span class=nv>PATH</span><span class=o>=</span><span class=nv>$HADOOP_HOME</span>/bin:<span class=nv>$JAVA_HOME</span>/bin:<span class=nv>$PATH</span>
</code></pre></td></tr></table></div></div><p>修改一键部署脚本：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>vi conf/config/install_config.conf
</code></pre></td></tr></table></div></div><p>内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span><span class=lnt>78
</span><span class=lnt>79
</span><span class=lnt>80
</span><span class=lnt>81
</span><span class=lnt>82
</span><span class=lnt>83
</span><span class=lnt>84
</span><span class=lnt>85
</span><span class=lnt>86
</span><span class=lnt>87
</span><span class=lnt>88
</span><span class=lnt>89
</span><span class=lnt>90
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=c1># 这里填 mysql or postgresql</span>
<span class=nv>dbtype</span><span class=o>=</span><span class=s2>&#34;mysql&#34;</span>

<span class=c1># 数据库连接地址</span>
<span class=nv>dbhost</span><span class=o>=</span><span class=s2>&#34;localhost:3306&#34;</span>

<span class=c1># 数据库名</span>
<span class=nv>dbname</span><span class=o>=</span><span class=s2>&#34;dolphinscheduler&#34;</span>

<span class=c1># 数据库用户名，此处需要修改为上面设置的{user}具体值</span>
<span class=nv>username</span><span class=o>=</span><span class=s2>&#34;dolphinscheduler&#34;</span>    

<span class=c1># 数据库密码, 如果有特殊字符，请使用\转义，需要修改为上面设置的{password}具体值</span>
<span class=nv>password</span><span class=o>=</span><span class=s2>&#34;changeme&#34;</span>

<span class=c1>#Zookeeper地址，单机本机是localhost:2181，记得把2181端口带上</span>
<span class=nv>zkQuorum</span><span class=o>=</span><span class=s2>&#34;hadoop-node1.nil.ml:2181,hadoop-node2.nil.ml:2181,hadoop-node3.nil.ml:2181&#34;</span>

<span class=c1>#将DS安装到哪个目录，如: /opt/soft/dolphinscheduler，不同于现在的目录</span>
<span class=nv>installPath</span><span class=o>=</span><span class=s2>&#34;/opt/dolphinscheduler&#34;</span>

<span class=c1>#使用哪个用户部署，使用第3节创建的用户</span>
<span class=nv>deployUser</span><span class=o>=</span><span class=s2>&#34;dolphinscheduler&#34;</span>

<span class=c1># 邮件配置，以qq邮箱为例</span>
<span class=c1># 邮件协议</span>
<span class=nv>mailProtocol</span><span class=o>=</span><span class=s2>&#34;SMTP&#34;</span>

<span class=c1># 邮件服务地址</span>
<span class=nv>mailServerHost</span><span class=o>=</span><span class=s2>&#34;smtp.qq.com&#34;</span>

<span class=c1># 邮件服务端口</span>
<span class=nv>mailServerPort</span><span class=o>=</span><span class=s2>&#34;25&#34;</span>

<span class=c1># mailSender和mailUser配置成一样即可</span>
<span class=c1># 发送者</span>
<span class=nv>mailSender</span><span class=o>=</span><span class=s2>&#34;xxx@qq.com&#34;</span>

<span class=c1># 发送用户</span>
<span class=nv>mailUser</span><span class=o>=</span><span class=s2>&#34;xxx@qq.com&#34;</span>

<span class=c1># 邮箱密码</span>
<span class=nv>mailPassword</span><span class=o>=</span><span class=s2>&#34;xxx&#34;</span>

<span class=c1># TLS协议的邮箱设置为true，否则设置为false</span>
<span class=nv>starttlsEnable</span><span class=o>=</span><span class=s2>&#34;true&#34;</span>

<span class=c1># 开启SSL协议的邮箱配置为true，否则为false。注意: starttlsEnable和sslEnable不能同时为true</span>
<span class=nv>sslEnable</span><span class=o>=</span><span class=s2>&#34;false&#34;</span>

<span class=c1># 邮件服务地址值，参考上面 mailServerHost</span>
<span class=nv>sslTrust</span><span class=o>=</span><span class=s2>&#34;smtp.qq.com&#34;</span>

<span class=c1># 业务用到的比如sql等资源文件上传到哪里，可以设置：HDFS,S3,NONE，单机如果想使用本地文件系统，请配置为HDFS，因为HDFS支持本地文件系统；如果不需要资源上传功能请选择NONE。强调一点：使用本地文件系统不需要部署hadoop</span>
<span class=nv>resourceStorageType</span><span class=o>=</span><span class=s2>&#34;HDFS&#34;</span>

<span class=c1># 这里以保存到本地文件系统为例</span>
<span class=c1>#注：但是如果你想上传到HDFS的话，NameNode启用了HA，则需要将hadoop的配置文件core-site.xml和hdfs-site.xml放到conf目录下，本例即是放到/opt/dolphinscheduler/conf下面，并配置namenode cluster名称；如果NameNode不是HA,则修改为具体的ip或者主机名即可</span>
<span class=nv>defaultFS</span><span class=o>=</span><span class=s2>&#34;file:///hadoop-cluster/dolphinscheduler&#34;</span>    <span class=c1>#hdfs://{具体的ip/主机名}:8020</span>

<span class=c1># 如果没有使用到Yarn,保持以下默认值即可；如果ResourceManager是HA，则配置为ResourceManager节点的主备ip或者hostname,比如&#34;192.168.xx.xx,192.168.xx.xx&#34;;如果是单ResourceManager请配置yarnHaIps=&#34;&#34;即可</span>
<span class=c1># 注：依赖于yarn执行的任务，为了保证执行结果判断成功,需要确保yarn信息配置正确。</span>
<span class=nv>yarnHaIps</span><span class=o>=</span><span class=s2>&#34;hadoop-node1.nil.ml,hadoop-node2.nil.ml,hadoop-node3.nil.ml&#34;</span>

<span class=c1># 如果ResourceManager是HA或者没有使用到Yarn保持默认值即可；如果是单ResourceManager，请配置真实的ResourceManager主机名或者ip</span>
<span class=nv>singleYarnIp</span><span class=o>=</span><span class=s2>&#34;yrc&#34;</span>

<span class=c1># 资源上传根路径,支持HDFS和S3,由于hdfs支持本地文件系统，需要确保本地文件夹存在且有读写权限</span>
<span class=nv>resourceUploadPath</span><span class=o>=</span><span class=s2>&#34;/data/dolphinscheduler/&#34;</span>

<span class=c1># 具备权限创建resourceUploadPath的用户</span>
<span class=nv>hdfsRootUser</span><span class=o>=</span><span class=s2>&#34;hdfs&#34;</span>

<span class=c1>#在哪些机器上部署DS服务，本机选localhost</span>
<span class=nv>ips</span><span class=o>=</span><span class=s2>&#34;localhost&#34;</span>

<span class=c1>#ssh端口,默认22</span>
<span class=nv>sshPort</span><span class=o>=</span><span class=s2>&#34;22&#34;</span>

<span class=c1>#master服务部署在哪台机器上</span>
<span class=nv>masters</span><span class=o>=</span><span class=s2>&#34;localhost&#34;</span>

<span class=c1>#worker服务部署在哪台机器上,并指定此worker属于哪一个worker组,下面示例的default即为组名</span>
<span class=nv>workers</span><span class=o>=</span><span class=s2>&#34;localhost:default&#34;</span>

<span class=c1>#报警服务部署在哪台机器上</span>
<span class=nv>alertServer</span><span class=o>=</span><span class=s2>&#34;localhost&#34;</span>

<span class=c1>#后端api服务部署在在哪台机器上</span>
<span class=nv>apiServers</span><span class=o>=</span><span class=s2>&#34;localhost&#34;</span>
</code></pre></td></tr></table></div></div><p>创建目录：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo mkdir -pv /data/dolphinscheduler
sudo chown -R dolphinscheduler:dolphinscheduler /data/dolphinscheduler
</code></pre></td></tr></table></div></div><p>在部署之前先切换一下默认的 <code>/bin/sh</code>连接位置：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo rm /bin/sh
sudo ln -s /usr/bin/bash /bin/sh
</code></pre></td></tr></table></div></div><p>一键部署：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sh install.sh
</code></pre></td></tr></table></div></div><p>登录地址：hadoop-node1.nil.ml:12345/dolphinscheduler
默认用户：admin
默认密码：dolphinscheduler123</p><h2 id=监控>监控</h2><p>Prometheus
LogicMonitor
Dynatrace</p><h2 id=日志处理>日志处理</h2><h2 id=后记>后记</h2><h2 id=参考链接>参考链接</h2><p>(How to Install and Configure Hadoop on Ubuntu 20.04)[https://tecadmin.net/install-hadoop-on-ubuntu-20-04/]
(Configuring Ports)[https://ambari.apache.org/1.2.3/installing-hadoop-using-ambari/content/reference_chap2.html]
<a href=https://developpaper.com/building-hadoop-high-availability-cluster-based-on-zookeeper/>https://developpaper.com/building-hadoop-high-availability-cluster-based-on-zookeeper/</a>
<a href=https://tutorials.freshersnwo.com/hadoop-tutorial/hadoop-high-availability/>https://tutorials.freshersnwo.com/hadoop-tutorial/hadoop-high-availability/</a>
<a href=https://www.xenonstack.com/insights/apache-zookeeper-security/>https://www.xenonstack.com/insights/apache-zookeeper-security/</a>
<a href=https://zookeeper.apache.org/doc/r3.6.0/zookeeperAdmin.html#sc_authOptions>https://zookeeper.apache.org/doc/r3.6.0/zookeeperAdmin.html#sc_authOptions</a>
<a href=https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/SecureContainer.html>https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/SecureContainer.html</a>
<a href=https://docs.confluent.io/platform/current/security/zk-security.html>https://docs.confluent.io/platform/current/security/zk-security.html</a>
<a href=https://docs.cloudera.com/runtime/7.2.0/zookeeper-security/topics/zookeeper-configure-client-shell-kerberos-authentication.html>https://docs.cloudera.com/runtime/7.2.0/zookeeper-security/topics/zookeeper-configure-client-shell-kerberos-authentication.html</a>
<a href=https://www.xenonstack.com/insights/apache-zookeeper-security/>https://www.xenonstack.com/insights/apache-zookeeper-security/</a>
<a href=https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SecureMode.html>https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SecureMode.html</a>
<a href=https://zhuanlan.zhihu.com/p/99398378>https://zhuanlan.zhihu.com/p/99398378</a></p></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>Yafa Xena</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2021-02-01</span></p><p class=copyright-item><span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><footer class=post-footer><div class=post-tags><a href=/tags/hadoop/>hadoop</a>
<a href=/tags/linux/>linux</a></div><nav class=post-nav><a class=prev href=/post/bash/><i class="iconfont icon-left"></i><span class="prev-text nav-default">Bash</span>
<span class="prev-text nav-mobile">Prev</span></a>
<a class=next href=/post/vsftp/><span class="next-text nav-default">Vsftp服务器</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:yafa-xena@protonmail.com class="iconfont icon-email" title=email></a><a href=https://twitter.com/XenaYafa class="iconfont icon-twitter" title=twitter></a><a href=https://github.com/yafa-xena/ class="iconfont icon-github" title=github></a><a href=https://www.yafa.moe/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span>
<span class=copyright-year>&copy;
2021<span class=heart><i class="iconfont icon-heart"></i></span><span>Yafa Xena</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=/js/main.min.c12618f9a600c40bd024996677e951e64d3487006775aeb22e200c990006c5c7.js></script><script type=text/javascript>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],tags:'ams',}};</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-194993895-1','auto');ga('set','anonymizeIp',true);ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></body></html>