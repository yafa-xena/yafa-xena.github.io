<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Hadoop - Yafa Xena's blog</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="Yafa Xena"><meta name=description content="前言 最近准备接手一些大数据的东西了，在接手之前肯定是需要对大数据的生态以及常用的软件配置使用等等都要有一个比较基础的认知，为此我会写几篇文章关于大数据的那些事情，这篇文章就是关于hadoop的。"><meta name=keywords content="Linux,Emacs,Python,Golang,Devops,Zabbix,Kubernetes,Gentoo"><meta name=generator content="Hugo 0.79.1 with theme even"><link rel=canonical href=https://www.yafa.moe/post/hadoop/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link href=/sass/main.min.b5a744db6de49a86cadafb3b70f555ab443f83c307a483402259e94726b045ff.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="Hadoop"><meta property="og:description" content="前言
最近准备接手一些大数据的东西了，在接手之前肯定是需要对大数据的生态以及常用的软件配置使用等等都要有一个比较基础的认知，为此我会写几篇文章关于大数据的那些事情，这篇文章就是关于hadoop的。"><meta property="og:type" content="article"><meta property="og:url" content="https://www.yafa.moe/post/hadoop/"><meta property="article:published_time" content="2021-02-01T16:51:57+08:00"><meta property="article:modified_time" content="2021-02-01T16:51:57+08:00"><meta itemprop=name content="Hadoop"><meta itemprop=description content="前言
最近准备接手一些大数据的东西了，在接手之前肯定是需要对大数据的生态以及常用的软件配置使用等等都要有一个比较基础的认知，为此我会写几篇文章关于大数据的那些事情，这篇文章就是关于hadoop的。"><meta itemprop=datePublished content="2021-02-01T16:51:57+08:00"><meta itemprop=dateModified content="2021-02-01T16:51:57+08:00"><meta itemprop=wordCount content="7679"><meta itemprop=keywords content="hadoop,linux,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Hadoop"><meta name=twitter:description content="前言
最近准备接手一些大数据的东西了，在接手之前肯定是需要对大数据的生态以及常用的软件配置使用等等都要有一个比较基础的认知，为此我会写几篇文章关于大数据的那些事情，这篇文章就是关于hadoop的。"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>Yafa-Xena</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/todo><li class=mobile-menu-item>Todo</li></a><a href=/about><li class=mobile-menu-item>About</li></a><a href=/product><li class=mobile-menu-item>Product</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>Yafa-Xena</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/todo>Todo</a></li><li class=menu-item><a class=menu-item-link href=/about>About</a></li><li class=menu-item><a class=menu-item-link href=/product>Product</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>Hadoop</h1><div class=post-meta><span class=post-time>2021-02-01</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#前言>前言</a></li><li><a href=#hadoop介绍>hadoop介绍</a></li><li><a href=#高可用原理介绍>高可用原理介绍</a><ul><li><a href=#hdfs-ha原理>HDFS HA原理</a></li></ul></li><li><a href=#软件兼容表>软件兼容表</a></li><li><a href=#环境准备>环境准备</a><ul><li><a href=#节点配置>节点配置</a></li><li><a href=#集群规划>集群规划</a></li><li><a href=#添加管理用户>添加管理用户</a></li><li><a href=#端口说明>端口说明</a></li><li><a href=#虚拟机网络>虚拟机网络</a></li><li><a href=#ntp配置>NTP配置</a></li><li><a href=#安装jdk环境>安装JDK环境</a></li></ul></li><li><a href=#安装zookeeper>安装zookeeper</a><ul><li><a href=#配置zookeeper>配置zookeeper</a></li><li><a href=#启动zookeeper并检查状态>启动zookeeper并检查状态</a></li></ul></li><li><a href=#安装hadoop>安装hadoop</a><ul><li><a href=#免密>免密</a></li><li><a href=#下载hadoop>下载hadoop</a></li><li><a href=#启动>启动</a></li><li><a href=#hadoop-服务-systemd-service-文件>hadoop 服务 systemd service 文件</a></li><li><a href=#hdfs-测试>HDFS 测试</a></li><li><a href=#ha验证>HA验证</a></li><li><a href=#为每个服务编写systemd-services-文件>为每个服务编写systemd services 文件</a></li></ul></li><li><a href=#海豚调度>海豚调度</a><ul><li><a href=#安装数据库>安装数据库</a></li><li><a href=#安装和配置海豚调度>安装和配置海豚调度</a></li></ul></li><li><a href=#日志处理>日志处理</a></li><li><a href=#安全加固>安全加固</a></li><li><a href=#后记>后记</a></li><li><a href=#参考链接>参考链接</a></li></ul></li></ul></nav></div></div><div class=post-content><h2 id=前言>前言</h2><p>最近准备接手一些大数据的东西了，在接手之前肯定是需要对大数据的生态以及常用的软件配置使用等等都要有一个比较基础的认知，为此我会写几篇文章关于大数据的那些事情，这篇文章就是关于hadoop的。</p><h2 id=hadoop介绍>hadoop介绍</h2><p>hadoop 是apache基金会开源出来的并行处理工具
Hbase：十一个nosql的数据库，类似于mongodb
HDFS： hadoop distribut file system, hadoop的分布式文件系统
Zookeeper是分布式管理协助框架，Zookeeper集群在这里用于保证Hadoop集群的高可用。</p><h2 id=高可用原理介绍>高可用原理介绍</h2><p>Zookeeper集群能够保证NameNode服务高可用省得原理是：Hadoop集群中有2个NameNode服务，两个Namenode服务都定时给Zookeeper发送心跳，告诉Zookeeper我还或者，可以提供服务，单个时间点只有一个是Action的状态，另一个是Standby状态，一旦Zookeeper检测不到Action NameNode发送来的心跳之后，就会切换到Standby状态的NameNode上，将它设置为Action NameNode的状态，以此来达到NameNode高可用的目的。</p><p>Zookeeper集群本身也可以保证自身的高可用，Zookeeper集群中的各个节点分为Leader、Follower两个。</p><p>当写数据的时候需要先写入Leader节点，Leader写入之后再通知Follower写入。</p><p>客户端读取数据的时候，因为数据都是一样的，可以从任意一台机器上进行读取数据。</p><p>Zookeeper当Leader节点发生故障的时候，就会进行选举流程。这个流程是：集群中任何一台机器发现集群中没有Leader的时候，就会推荐自己为Leader，其他机器来发起投票同意，当超过一半的机器同意它为Leader的时候，选举结束。</p><p>所以Zookeeper集群中的机器数量必须是奇数这样就算是Leader拒绝服务也会很快选出新的Leader，从而保证了Zookeeper集群的高可用性。</p><p>ZK的三个角色</p><ul><li>Leader</li><li>Follower</li><li>Observer</li></ul><h3 id=hdfs-ha原理>HDFS HA原理</h3><p>单点的NameNode的缺陷在于单点故障的问题，如果NameNode不可用会导致整个HDFS系统不可用，所以需要设计高可用的HDFS来解决NamNode单点故障的问题。及觉得方法就是再HDFS集群中设置多个NameNode节点，但是一旦引入多个NameNode就有一些问题需要解决。</p><ul><li>如何保证NameNode内存中元数据一致，并保证编辑日志文件的安全性。</li><li>多个NameNode如何进行协作</li><li>客户端如何能够找到可用的NameNode</li><li>如何保证任意时刻只有一个NameNode处于对外服务的状态。</li></ul><p>以下是引入ZK的解决方法</p><ul><li>对于保证NameNode元数据的一致性和编辑日志的安全性，采用Zookeeper来存储日志文件。</li><li>两个NameNode一个是Active状态的，一个是Standby状态的，一个时间点上之能够有一个为Active状态的NameNode。</li></ul><h2 id=软件兼容表>软件兼容表</h2><p>大数据的生态比较复杂，像是常用的hbase、hive都是对安装的软件是有些要求的，这里我总结了一个表格方便去查看和使用。以避免因为版本不兼容带来的各种各样的问题。</p><p>hadoop以及必要的组件版本如下：</p><table><thead><tr><th>软件名</th><th>版本</th><th>备注</th></tr></thead><tbody><tr><td>hadoop</td><td>3.3.0</td><td>stable</td></tr><tr><td>zookeeper</td><td>3.6.3</td><td>stable</td></tr><tr><td>hive</td><td>3.1.2</td><td>stable</td></tr><tr><td>hbase</td><td></td><td></td></tr></tbody></table><h2 id=环境准备>环境准备</h2><p>这里准备了3台虚拟机来做这次的实验：</p><table><thead><tr><th>主机名</th><th>OS</th><th>ip</th><th>角色</th></tr></thead><tbody><tr><td>hadoop-node1</td><td>Ubuntu18.04</td><td>192.168.122.40</td><td>master</td></tr><tr><td>hadoop-node2</td><td>Ubuntu18.04</td><td>192.168.122.41</td><td>worker1</td></tr><tr><td>hadoop-node3</td><td>Ubuntu18.04</td><td>192.168.122.42</td><td>worker2</td></tr></tbody></table><p>hadoop以及必要的组件版本如下：</p><table><thead><tr><th>软件名</th><th>版本</th><th>备注</th></tr></thead><tbody><tr><td>hadoop</td><td>3.3.0</td><td>stable</td></tr><tr><td>zookeeper</td><td>3.6.3</td><td>stable</td></tr><tr><td>hive</td><td>3.1.2</td><td>stable</td></tr><tr><td>hbase</td><td></td><td></td></tr></tbody></table><p>海豚调度版本如下：</p><table><thead><tr><th>软件名</th><th>版本</th><th>备注</th></tr></thead><tbody><tr><td>apache-dolphinscheduler</td><td>1.3.6</td><td>latest-bin</td></tr></tbody></table><p>进程描述：</p><table><thead><tr><th>描述</th><th>hadoop-node1</th><th>hadoop-node2</th><th>hadoop-node3</th></tr></thead><tbody><tr><td>HDFS主</td><td>NameNode</td><td>NameNode</td><td></td></tr><tr><td>HDFS从</td><td>DateNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td>Yarn主</td><td>ResourceManager</td><td>Resourcemanager</td><td></td></tr><tr><td>Yarn从</td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td>Hbase主</td><td>HMaster</td><td>Hmaster</td><td></td></tr><tr><td>Hbase从</td><td>HRegionServer</td><td>HRegionserver</td><td>HRegionserver</td></tr><tr><td>Zookeeper进程</td><td>QuorumPeerMain</td><td>Quorumpeermain</td><td>Quorumpeermain</td></tr><tr><td>NameNode数据同步</td><td>JournalNode</td><td>JournalNode</td><td>Journalnode</td></tr><tr><td>主备切换</td><td>DFSZKFailoverController</td><td>DFSzkfailovercontroller</td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr></tbody></table><h3 id=节点配置>节点配置</h3><p>关闭ipv6</p><p>在<code>/etc/sysctl.conf</code>文件后追加内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>net.ipv6.conf.all.disable_ipv6<span class=o>=</span><span class=m>1</span>
net.ipv6.conf.default.disable_ipv6<span class=o>=</span><span class=m>1</span>
net.ipv6.conf.lo.disable_ipv6<span class=o>=</span><span class=m>1</span>
</code></pre></td></tr></table></div></div><p>同时还需要修改grub</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nv>GRUB_CMDLINE_LINUX</span><span class=o>=</span><span class=s2>&#34;crashkernel=auto ... ipv6.disable=1&#34;</span>
</code></pre></td></tr></table></div></div><p>更新grub：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>update-grub2
</code></pre></td></tr></table></div></div><h3 id=集群规划>集群规划</h3><p>节点以及角色的对应表格：</p><table><thead><tr><th>vm name</th><th>role</th></tr></thead><tbody><tr><td>ntp</td><td>ntp server</td></tr><tr><td>dns</td><td>dns server</td></tr><tr><td>kerberos-server</td><td>kerberos server</td></tr><tr><td>hadoop-node1.nil.ml</td><td>namenode-master, datanode, journalnode, nodemanager, zookeeper, DFSZKFailoverController</td></tr><tr><td>hadoop-node2.nil.ml</td><td>Namenode-Slave, DFSzkfailovercontroller, Resourcemanager-master, Datanode, JournalNode, NodeManager, Zookeeper</td></tr><tr><td>hadoop-node3.nil.ml</td><td>Resourcemanager-Slave, DataNode, JournalNode, Nodemanager, Zookeeper</td></tr></tbody></table><p>所用到用户如下表：</p><table><thead><tr><th>用户名</th><th>权限</th></tr></thead><tbody><tr><td><code>user</code></td><td>默认管理员</td></tr><tr><td><code>zookeeper</code></td><td>普通用户（启动zk）</td></tr><tr><td><code>dolphinscheduler</code></td><td>dolphinscheduler 管理用户</td></tr><tr><td><code>hdfs</code></td><td>HDFS 运行用户</td></tr><tr><td><code>hive</code></td><td>Hive 运行用户</td></tr><tr><td><code>yarn</code></td><td>Yarn运行用户</td></tr><tr><td><code>mapred</code></td><td>MR用户</td></tr><tr><td><code>spark</code></td><td>spark 用户</td></tr></tbody></table><p>使用到的组：</p><table><thead><tr><th>组名</th><th>用户成员</th><th>说明</th></tr></thead><tbody><tr><td>hadoop</td><td>hdfs, yarn, mapred, dolphinscheduler</td><td>大数据成员</td></tr></tbody></table><h3 id=添加管理用户>添加管理用户</h3><p>sudoer 设置：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>%sudo <span class=nv>ALL</span><span class=o>=(</span>ALL<span class=o>)</span> NOPASSWD: ALL
</code></pre></td></tr></table></div></div><p>创建<code>hadoop</code>组：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo groupadd hadoop
</code></pre></td></tr></table></div></div><p>创建hdfs账户：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo useradd -m hdfs -G hadoop -s /bin/bash
<span class=nb>echo</span> <span class=s2>&#34;hdfs:changeme&#34;</span> <span class=p>|</span> sudo chpasswd
</code></pre></td></tr></table></div></div><p>创建yarn账户：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo useradd -m yarn -G hadoop -s /bin/bash
<span class=nb>echo</span> <span class=s2>&#34;yarn:changeme&#34;</span> <span class=p>|</span> sudo chpasswd
</code></pre></td></tr></table></div></div><p>创建mapred账户：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo useradd -m mapred -G hadoop -s /bin/bash
<span class=nb>echo</span> <span class=s2>&#34;mapred:changeme&#34;</span> <span class=p>|</span> sudo chpasswd
</code></pre></td></tr></table></div></div><p>创建hive账户：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo useradd -m hive -G hadoop -s /bin/bash
<span class=nb>echo</span> <span class=s2>&#34;hive:changeme&#34;</span> <span class=p>|</span> sudo chpasswd
</code></pre></td></tr></table></div></div><p>创建hbase账户：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo useradd -m hbase -G hadoop -s /bin/bash
<span class=nb>echo</span> <span class=s2>&#34;hbase:changeme&#34;</span> <span class=p>|</span> sudo chpasswd
</code></pre></td></tr></table></div></div><p>创建dolphinscheduler账户：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo useradd -m dolphinscheduler -G sudo,hadoop -s /bin/bash
<span class=nb>echo</span> <span class=s2>&#34;dolphinscheduler:changeme&#34;</span> <span class=p>|</span> sudo chpasswd
</code></pre></td></tr></table></div></div><p>添加<code>zk</code>的管理用户：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo useradd -m zookeeper -s /bin/bash
<span class=nb>echo</span> <span class=s2>&#34;zookeeper:changeme&#34;</span> <span class=p>|</span> sudo chpasswd
</code></pre></td></tr></table></div></div><p>创建的目录以及对应的访问权限如下表：</p><table><thead><tr><th>目录</th><th>权限</th><th>备注</th></tr></thead><tbody><tr><td><code>/opt/hadoop/</code></td><td>user : rwx</td><td>hadoop程序</td></tr><tr><td><code>/opt/packages</code></td><td>user: rwx</td><td>所有包存放的地方</td></tr><tr><td><code>/opt/zookeeper</code></td><td>zookeeper:rwx</td><td></td></tr><tr><td><code>/opt/hbase</code></td><td>hbase:rwx</td><td></td></tr><tr><td><code>/opt/hive</code></td><td>hive:rwx</td><td></td></tr><tr><td><code>/opt/dolphinscheduler</code></td><td>dolphinscheduler:rwx</td><td></td></tr><tr><td><code>/opt/spark</code></td><td>spark:rwx</td><td></td></tr><tr><td><code>/opt/hdfs</code></td><td>hdfs:hadoop</td><td>hdfs namenode和datanode存放数据的地方</td></tr></tbody></table><p>创建并修改权限：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo mkdir -pv /opt/packages
sudo chown -R user /opt/packages
sudo mkdir -pv /opt/hdfs/<span class=o>{</span>namenode,datanode,journalnode<span class=o>}</span>
sudo chown -R hdfs:hadoop /opt/hdfs/
</code></pre></td></tr></table></div></div><h3 id=端口说明>端口说明</h3><p>端口以及对应的服务如下表：</p><table><thead><tr><th>端口</th><th>服务</th><th>说明</th></tr></thead><tbody><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table><ul><li>2888 内部通讯</li><li>3888 选举leader的端口</li></ul><h3 id=虚拟机网络>虚拟机网络</h3><p>虚拟机的网络分为：192.168.122.0/24</p><p>配置虚拟机的ip和主机名：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hostnamectl set-hostname hadoop-node1.nil.ml
</code></pre></td></tr></table></div></div><p>备份原来的网络配置文件：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>cp -v /etc/netplan/00-installer-config.yaml /etc/netplan/00-installer-config.yaml.bak
</code></pre></td></tr></table></div></div><p>修改配置文件：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-yaml data-lang=yaml><span class=c># This is the network config written by &#39;subiquity&#39;</span><span class=w>
</span><span class=w></span><span class=nt>network</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>ethernets</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>enp1s0</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>dhcp4</span><span class=p>:</span><span class=w> </span><span class=kc>no</span><span class=w>
</span><span class=w>      </span><span class=nt>addresses</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=m>192.168.122.40</span><span class=l>/24]</span><span class=w>
</span><span class=w>      </span><span class=nt>gateway4</span><span class=p>:</span><span class=w> </span><span class=m>192.168.122.1</span><span class=w>
</span><span class=w>      </span><span class=nt>nameservers</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>addresses</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=m>192.168.122.1</span><span class=p>,</span><span class=m>114.114.114.114</span><span class=p>]</span><span class=w>
</span><span class=w>      </span><span class=nt>optional</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>  </span><span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>
</span></code></pre></td></tr></table></div></div><p>保存并退出之后应用：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>netplan apply
</code></pre></td></tr></table></div></div><p>设置主机名：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hostnamectl set-hostname hadoop-node1.nil.ml
</code></pre></td></tr></table></div></div><p>hadoop-node2</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hostnamectl set-hostname hadoop-node2.nil.ml
</code></pre></td></tr></table></div></div><p>hadoop-node3</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hostnamectl set-hostname hadoop-node3.nil.ml
</code></pre></td></tr></table></div></div><h3 id=ntp配置>NTP配置</h3><p>在集群里面节点的时间同步是非常重要的部分，这里来配置一下时间同步服务器。</p><p>首先调整一下时区：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo rm -f /etc/localtime
sudo ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
</code></pre></td></tr></table></div></div><p>安装ntp服务器</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo apt-get install ntp -y
</code></pre></td></tr></table></div></div><p>配置ntp服务器。</p><p>这里需要将其他的pool注释掉，然后添加这一条：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>pool ntp.aliyun.com iburst
</code></pre></td></tr></table></div></div><p>保存退出之后将ntp服务加入到开机启动项中并启动：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo systemctl <span class=nb>enable</span> ntp --now
</code></pre></td></tr></table></div></div><p>检查时间是否是CTS时区</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>date
</code></pre></td></tr></table></div></div><h3 id=安装jdk环境>安装JDK环境</h3><p>这里安装的是<a href=http://openjdk.java.net/>openjdk</a>8的版本：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo apt-get update <span class=o>&amp;&amp;</span> sudo apt-get install openjdk-8-jdk -y
</code></pre></td></tr></table></div></div><p>配置一下shell的变量，使得用户可以直接使用java。</p><p>编辑<code>/etc/profile</code>文件</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo vi /etc/profile
</code></pre></td></tr></table></div></div><p>在末尾添加：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>export</span> <span class=nv>JAVA_HOME</span><span class=o>=</span>/usr/lib/jvm/java-1.8.0-openjdk-amd64/
</code></pre></td></tr></table></div></div><p>保存退出之后，运行：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>source</span> /etc/profile <span class=o>&amp;&amp;</span> java -version
</code></pre></td></tr></table></div></div><p>如果成功输出</p><h2 id=安装zookeeper>安装zookeeper</h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>cd</span> /opt/packages/
sudo tar -xf apache-zookeeper-3.6.3-bin.tar.gz
sudo mv apache-zookeeper-3.6.3-bin /opt/zookeeper
</code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo mkdir -pv /opt/zookeeper/data
</code></pre></td></tr></table></div></div><p>更改权限：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo chown -R zookeeper /opt/zookeeper
</code></pre></td></tr></table></div></div><h3 id=配置zookeeper>配置zookeeper</h3><p>切换到zk用户：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo su - zookeeper
</code></pre></td></tr></table></div></div><p>创建并编辑配置文件<code>/opt/zookeeper/conf/zoo.cfg</code>：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>vi /opt/zookeeper/conf/zoo.cfg
</code></pre></td></tr></table></div></div><p>内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-fallback data-lang=fallback>tickTime=2000
initLimit=10
syncLimit=5
dataDir=/opt/zookeeper/data
clientPort=2181
server.1=hadoop-node1.nil.ml:2888:3888
server.2=hadoop-node2.nil.ml:2888:3888
server.3=hadoop-node3.nil.ml:2888:3888
</code></pre></td></tr></table></div></div><p>配置id：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>echo</span> <span class=s2>&#34;1&#34;</span> &gt; /opt/zookeeper/data/myid
</code></pre></td></tr></table></div></div><h3 id=启动zookeeper并检查状态>启动zookeeper并检查状态</h3><p>启动：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>/opt/zookeeper/bin/zkServer.sh start
</code></pre></td></tr></table></div></div><p>查看服务运行状态：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>./bin/zkServer.sh status
</code></pre></td></tr></table></div></div><p>为了方便管理我们这里为zookeeper创建一个systemd的service。</p><p>创建并编辑<code>/lib/systemd/system/zookeeper.service</code></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo vi /lib/systemd/system/zookeeper.service
</code></pre></td></tr></table></div></div><p>内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=o>[</span>Unit<span class=o>]</span>
<span class=nv>Description</span><span class=o>=</span>Zookeeper Daemon
<span class=nv>Documentation</span><span class=o>=</span>http://zookeeper.apache.org
<span class=nv>Requires</span><span class=o>=</span>network.target
<span class=nv>After</span><span class=o>=</span>network.target

<span class=o>[</span>Service<span class=o>]</span>
<span class=nv>Type</span><span class=o>=</span>forking
<span class=nv>WorkingDirectory</span><span class=o>=</span>/opt/zookeeper
<span class=nv>User</span><span class=o>=</span>zookeeper
<span class=nv>Group</span><span class=o>=</span>zookeeper
<span class=nv>ExecStart</span><span class=o>=</span>/opt/zookeeper/bin/zkServer.sh start /opt/zookeeper/conf/zoo.cfg
<span class=nv>ExecStop</span><span class=o>=</span>/opt/zookeeper/bin/zkServer.sh stop /opt/zookeeper/conf/zoo.cfg
<span class=nv>ExecReload</span><span class=o>=</span>/opt/zookeeper/bin/zkServer.sh restart /opt/zookeeper/conf/zoo.cfg
<span class=nv>TimeoutSec</span><span class=o>=</span><span class=m>30</span>
<span class=nv>Restart</span><span class=o>=</span>on-failure

<span class=o>[</span>Install<span class=o>]</span>
<span class=nv>WantedBy</span><span class=o>=</span>default.target
</code></pre></td></tr></table></div></div><p>启动服务：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo systemctl start zookeeper
</code></pre></td></tr></table></div></div><p>查看服务状态：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo systemctl status zookeeper
</code></pre></td></tr></table></div></div><p>加入到开机启动：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo systemctl <span class=nb>enable</span> zookeeper
</code></pre></td></tr></table></div></div><h2 id=安装hadoop>安装hadoop</h2><h3 id=免密>免密</h3><blockquote><p>这里使用<code>data</code>用户来做免密：</p></blockquote><p>首先切换到<code>data</code>用户：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo su - data
</code></pre></td></tr></table></div></div><p>生产密钥（三个节点都需要）：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>ssh-keygen
</code></pre></td></tr></table></div></div><p>这一步在<code>hadoop-node1</code>上操作：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>ssh-copy-id localhost
</code></pre></td></tr></table></div></div><p>这一步用于生成<code>.ssh/authorized_keys</code>文件。</p><p>接下来我们添加其他两个公钥：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>ssh-copy-id hadoop-node1.nil.ml
</code></pre></td></tr></table></div></div><p>最再将这个文件<code>.ssh/authorized_keys</code>传输到其他的两个点上。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>scp ~/.ssh/authorized_keys hadoop-node2.nil.ml:~/.ssh
scp ~/.ssh/authorized_keys hadoop-node3.nil.ml:~/.ssh
</code></pre></td></tr></table></div></div><p>验证一下是否成功：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>ssh hadoop-node2.nil.ml <span class=s2>&#34;hostname&#34;</span>
ssh hadoop-node3.nil.ml <span class=s2>&#34;hostname&#34;</span>
</code></pre></td></tr></table></div></div><p>hdfs用户免密码：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>
</code></pre></td></tr></table></div></div><p>yarn用户免密码：</p><p>mapred用户免密码：</p><h3 id=下载hadoop>下载hadoop</h3><p>这里下载我们需要的软件包</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>cd</span> /opt/packages
wget https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz 
</code></pre></td></tr></table></div></div><p>解压，并放到单独的目录中：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo tar -xf hadoop-3.3.0.tar.gz 
sudo mv  hadoop-3.3.0  /opt/hadoop
</code></pre></td></tr></table></div></div><p>更改权限：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo chown -R data:hadoop /opt/hadoop
</code></pre></td></tr></table></div></div><p>修改 <code>/etc/profile</code> 文件：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>export</span> <span class=nv>JAVA_HOME</span><span class=o>=</span>/usr/lib/jvm/java-1.8.0-openjdk-amd64/
<span class=nb>export</span> <span class=nv>HADOOP_HOME</span><span class=o>=</span>/opt/hadoop
<span class=nb>export</span> <span class=nv>HADOOP_INSTALL</span><span class=o>=</span><span class=nv>$HADOOP_HOME</span>
<span class=nb>export</span> <span class=nv>HADOOP_MAPRED_HOME</span><span class=o>=</span><span class=nv>$HADOOP_HOME</span>
<span class=nb>export</span> <span class=nv>HADOOP_COMMON_HOME</span><span class=o>=</span><span class=nv>$HADOOP_HOME</span>
<span class=nb>export</span> <span class=nv>HADOOP_HDFS_HOME</span><span class=o>=</span><span class=nv>$HADOOP_HOME</span>
<span class=nb>export</span> <span class=nv>HADOOP_YARN_HOME</span><span class=o>=</span><span class=nv>$HADOOP_HOME</span>
<span class=nb>export</span> <span class=nv>HADOOP_COMMON_LIB_NATIVE_DIR</span><span class=o>=</span><span class=nv>$HADOOP_HOME</span>/lib/native
<span class=nb>export</span> <span class=nv>PATH</span><span class=o>=</span><span class=nv>$PATH</span>:<span class=nv>$HADOOP_HOME</span>/sbin:<span class=nv>$HADOOP_HOME</span>/bin
<span class=nb>export</span> <span class=nv>HADOOP_OPTS</span><span class=o>=</span><span class=s2>&#34;-Djava.library.path=</span><span class=nv>$HADOOP_HOME</span><span class=s2>/lib/native&#34;</span>
</code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>source</span> /etc/profile
</code></pre></td></tr></table></div></div><p>修改hadoop的配置</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>vi <span class=nv>$HADOOP_HOME</span>/etc/hadoop/hadoop-env.sh
</code></pre></td></tr></table></div></div><p>添加<code>JAVA_HOME</code>的配置：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>export</span> <span class=nv>JAVA_HOME</span><span class=o>=</span>/usr/lib/jvm/java-1.8.0-openjdk-amd64/
</code></pre></td></tr></table></div></div><p>创建hdfs的文件夹：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo mkdir -pv /opt/hdfs/<span class=o>{</span>namenode,datanode,journalnode<span class=o>}</span>
sudo chown -R hdfs:hadoop /opt/hdfs/
</code></pre></td></tr></table></div></div><p>创建hadoop的临时目录：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>mkdir -pv /opt/hadoop/tmp
</code></pre></td></tr></table></div></div><p>在Master节点中，修改以下三个配置文件：</p><ul><li><a href=http://hadoop.apache.org/docs/r3.3.0/hadoop-project-dist/hadoop-common/core-default.xml><code>$HADOOP/etc/hadoop/core-site.xml</code></a></li><li><a href=http://hadoop.apache.org/docs/r3.1.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml><code>$HADOOP/etc/hadoop/hdfs-site.xml</code></a></li><li><a href=http://hadoop.apache.org/docs/r3.1.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml><code>$HADOOP/etc/hadoop/mapred-site.xml</code></a></li><li><a href=http://hadoop.apache.org/docs/r3.1.0/hadoop-yarn/hadoop-yarn-common/yarn-default.xml><code>$HADOOP/etc/yarn-site.xml</code></a></li><li><code>$HADOOP/etc/hadoop/workers</code></li></ul><p>修改<code>/opt/hadoop/etc/hadoop/core-site.xml</code>文件</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>nano /opt/hadoop/etc/hadoop/core-site.xml
</code></pre></td></tr></table></div></div><p>内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span><span class=lnt>78
</span><span class=lnt>79
</span><span class=lnt>80
</span><span class=lnt>81
</span><span class=lnt>82
</span><span class=lnt>83
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-xml data-lang=xml><span class=nt>&lt;configuration&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>hadoop.tmp.dir<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>/opt/hadoop/tmp<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;final&gt;</span>true<span class=nt>&lt;/final&gt;</span>
    <span class=c>&lt;!-- base for other temporary directories --&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
    <span class=nt>&lt;property&gt;</span>
        <span class=nt>&lt;name&gt;</span>fs.defaultFS<span class=nt>&lt;/name&gt;</span>
        <span class=nt>&lt;value&gt;</span>hdfs://hadoop-cluster/<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>io.file.buffer.size<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>131072<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=c>&lt;!-- zk ha --&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>ha.zookeeper.quorum<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>hadoop-node1.nil.ml:2181,hadoop-node2.nil.ml:2181,hadoop-node3.nil.ml:2181<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>ha.zookeeper.session-timeout.ms<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>1000<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;description&gt;</span> connection zookeeper timeout ms<span class=nt>&lt;/description&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>ipc.client.connect.max.retries<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>1000<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;description&gt;</span>Indicates the number of retries a client will make to establish a server connection.<span class=nt>&lt;/description&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>ipc.client.connect.retry.interval<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>10000<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;description&gt;</span>Indicates the number of milliseconds a client will wait for before retrying to establish a server connection.<span class=nt>&lt;/description&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=c>&lt;!-- enable kerberos auth --&gt;</span>
	<span class=nt>&lt;property&gt;</span>
        <span class=nt>&lt;name&gt;</span>hadoop.security.authorization<span class=nt>&lt;/name&gt;</span>
        <span class=nt>&lt;value&gt;</span>true<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=c>&lt;!-- use kerberos for auth --&gt;</span>
	<span class=nt>&lt;property&gt;</span>
        <span class=nt>&lt;name&gt;</span>hadoop.security.authentication<span class=nt>&lt;/name&gt;</span>
        <span class=nt>&lt;value&gt;</span>kerberos<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
        <span class=nt>&lt;name&gt;</span>hadoop.security.auth_to_local<span class=nt>&lt;/name&gt;</span>
        <span class=nt>&lt;value&gt;</span>
            RULE: [2:$1$@0](nn@.*NIL.ML)s/ .*/hdfs/
            RULE: [2:$1$@0](sn@.*NIL.ML)s/ .*/hdfs/
            RULE: [2:$1$@0](dn@.*NIL.ML)s/ .*/hdfs/
            RULE: [2:$1$@0](nm@.*NIL.ML)s/ .*/yarn/
            RULE: [2:$1$@0](rm@.*NIL.ML)s/ .*/yarn/
            RULE: [2:$1$@0](nn@.*NIL.ML)s/ .*/hdfs/
            RULE: [2:$1$@0](nn@.*NIL.ML)s/ .*/hdfs/
            DEFAULT
        <span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
        <span class=nt>&lt;name&gt;</span>hadoop.proxyuser.hive.hosts<span class=nt>&lt;/name&gt;</span>
        <span class=nt>&lt;value&gt;</span>*<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
        <span class=nt>&lt;name&gt;</span>hadoop.proxyuser.hive.groups<span class=nt>&lt;/name&gt;</span>
        <span class=nt>&lt;value&gt;</span>*<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
        <span class=nt>&lt;name&gt;</span>hadoop.proxyuser.hdfs.hosts<span class=nt>&lt;/name&gt;</span>
        <span class=nt>&lt;value&gt;</span>*<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
        <span class=nt>&lt;name&gt;</span>hadoop.proxyuser.hdfs.groups<span class=nt>&lt;/name&gt;</span>
        <span class=nt>&lt;value&gt;</span>*<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
        <span class=nt>&lt;name&gt;</span>hadoop.proxyuser.HTTP.hosts<span class=nt>&lt;/name&gt;</span>
        <span class=nt>&lt;value&gt;</span>*<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
        <span class=nt>&lt;name&gt;</span>hadoop.proxyuser.HTTP.groups<span class=nt>&lt;/name&gt;</span>
        <span class=nt>&lt;value&gt;</span>*<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
<span class=nt>&lt;/configuration&gt;</span>
</code></pre></td></tr></table></div></div><p><code>hdfs-site.xml</code></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>nano /opt/hadoop/etc/hadoop/hdfs-site.xml 
</code></pre></td></tr></table></div></div><p>hadoop-node1 节点：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>  1
</span><span class=lnt>  2
</span><span class=lnt>  3
</span><span class=lnt>  4
</span><span class=lnt>  5
</span><span class=lnt>  6
</span><span class=lnt>  7
</span><span class=lnt>  8
</span><span class=lnt>  9
</span><span class=lnt> 10
</span><span class=lnt> 11
</span><span class=lnt> 12
</span><span class=lnt> 13
</span><span class=lnt> 14
</span><span class=lnt> 15
</span><span class=lnt> 16
</span><span class=lnt> 17
</span><span class=lnt> 18
</span><span class=lnt> 19
</span><span class=lnt> 20
</span><span class=lnt> 21
</span><span class=lnt> 22
</span><span class=lnt> 23
</span><span class=lnt> 24
</span><span class=lnt> 25
</span><span class=lnt> 26
</span><span class=lnt> 27
</span><span class=lnt> 28
</span><span class=lnt> 29
</span><span class=lnt> 30
</span><span class=lnt> 31
</span><span class=lnt> 32
</span><span class=lnt> 33
</span><span class=lnt> 34
</span><span class=lnt> 35
</span><span class=lnt> 36
</span><span class=lnt> 37
</span><span class=lnt> 38
</span><span class=lnt> 39
</span><span class=lnt> 40
</span><span class=lnt> 41
</span><span class=lnt> 42
</span><span class=lnt> 43
</span><span class=lnt> 44
</span><span class=lnt> 45
</span><span class=lnt> 46
</span><span class=lnt> 47
</span><span class=lnt> 48
</span><span class=lnt> 49
</span><span class=lnt> 50
</span><span class=lnt> 51
</span><span class=lnt> 52
</span><span class=lnt> 53
</span><span class=lnt> 54
</span><span class=lnt> 55
</span><span class=lnt> 56
</span><span class=lnt> 57
</span><span class=lnt> 58
</span><span class=lnt> 59
</span><span class=lnt> 60
</span><span class=lnt> 61
</span><span class=lnt> 62
</span><span class=lnt> 63
</span><span class=lnt> 64
</span><span class=lnt> 65
</span><span class=lnt> 66
</span><span class=lnt> 67
</span><span class=lnt> 68
</span><span class=lnt> 69
</span><span class=lnt> 70
</span><span class=lnt> 71
</span><span class=lnt> 72
</span><span class=lnt> 73
</span><span class=lnt> 74
</span><span class=lnt> 75
</span><span class=lnt> 76
</span><span class=lnt> 77
</span><span class=lnt> 78
</span><span class=lnt> 79
</span><span class=lnt> 80
</span><span class=lnt> 81
</span><span class=lnt> 82
</span><span class=lnt> 83
</span><span class=lnt> 84
</span><span class=lnt> 85
</span><span class=lnt> 86
</span><span class=lnt> 87
</span><span class=lnt> 88
</span><span class=lnt> 89
</span><span class=lnt> 90
</span><span class=lnt> 91
</span><span class=lnt> 92
</span><span class=lnt> 93
</span><span class=lnt> 94
</span><span class=lnt> 95
</span><span class=lnt> 96
</span><span class=lnt> 97
</span><span class=lnt> 98
</span><span class=lnt> 99
</span><span class=lnt>100
</span><span class=lnt>101
</span><span class=lnt>102
</span><span class=lnt>103
</span><span class=lnt>104
</span><span class=lnt>105
</span><span class=lnt>106
</span><span class=lnt>107
</span><span class=lnt>108
</span><span class=lnt>109
</span><span class=lnt>110
</span><span class=lnt>111
</span><span class=lnt>112
</span><span class=lnt>113
</span><span class=lnt>114
</span><span class=lnt>115
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-xml data-lang=xml><span class=nt>&lt;configuration&gt;</span>
	    <span class=c>&lt;!-- 存储的副本数量 --&gt;</span>
        <span class=nt>&lt;property&gt;</span>
                <span class=nt>&lt;name&gt;</span>dfs.replication<span class=nt>&lt;/name&gt;</span>
				<span class=c>&lt;!-- if 3 datanode this value should be 2 --&gt;</span>
                <span class=nt>&lt;value&gt;</span>2<span class=nt>&lt;/value&gt;</span>
        <span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- namenode 数据的存放位置 --&gt;</span>
        <span class=nt>&lt;property&gt;</span>
                <span class=nt>&lt;name&gt;</span>dfs.name.dir<span class=nt>&lt;/name&gt;</span>
                <span class=nt>&lt;value&gt;</span>/opt/hdfs/namenode<span class=nt>&lt;/value&gt;</span>
        <span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- datanode 数据的存放位置 --&gt;</span>
        <span class=nt>&lt;property&gt;</span>
                <span class=nt>&lt;name&gt;</span>dfs.data.dir<span class=nt>&lt;/name&gt;</span>
                <span class=nt>&lt;value&gt;</span>/opt/hdfs/datanode<span class=nt>&lt;/value&gt;</span>
        <span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- JournalNode在本地磁盘存放数据的位置 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.journalnode.edits.dir<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>/opt/hdfs/journalnode<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		
		<span class=c>&lt;!-- secondary 节点信息 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.namenode.secondary.http-address<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>hadoop-node2.nil.ml:9001<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- 集群名称 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.nameservices<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>hadoop-cluster<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- namenode节点名称 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.ha.namenodes.hadoop-cluster<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>nn1,nn2<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- nn1 的rpc通信地址 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.namenode.rpc-address.hadoop-cluster.nn1<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>hadoop-node1.nil.ml:9000<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- nn2 的 RPC 通信地址 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.namenode.rpc-address.hadoop-cluster.nn2<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>hadoop-node2.nil.ml:9000<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- nn1的http 通信地址 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.namenode.http-address.hadoop-cluster.nn1<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>hadoop-node1.nil.ml:50070<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- nn2 的 http 通信地址 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.namenode.http-address.hadoop-cluster.nn2<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>hadoop-node2.nil.ml:50070<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- hdfs web --&gt;</span>
	    <span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.webhdfs.enabled<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>true<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- 指定 NameNode的edits元数据的共享存储位置也就是JournalNode列表url的配置 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.namenode.shared.edits.dir<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>qjournal://hadoop-node1.nil.ml:8485;hadoop-node2.nil.ml:8485;hadoop-node3.nil.ml:8485/hadoop-cluster<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- 开启NameNode失败自动切换 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.ha.automatic-failover.enabled<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>true<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- 指定失败之后的自动切换实现模式 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.client.failover.proxy.provider.hadoop-cluster<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- 配置隔离机制方法，多个机制使用换行分割：每个机制占用一行 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.ha.fencing.methods<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>
				sshfence
				shell(/bin/true)
			<span class=nt>&lt;/value&gt;</span>	
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- 使用 sshfence隔离机制的时候需要配置ssh免密登录 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>/home/data/.ssh/id_rsa<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- 配置sshfence隔离机制超时时间 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>30000<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>ha.failover-controller.cli-check.rpc-timeout.ms<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>60000<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- block access token --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
            <span class=nt>&lt;name&gt;</span>dfs.block.access.token.enable<span class=nt>&lt;/name&gt;</span>
            <span class=nt>&lt;value&gt;</span>true<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- namenode security config --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
            <span class=nt>&lt;name&gt;</span>dfs.namenode.kerberos.principal<span class=nt>&lt;/name&gt;</span>
            <span class=nt>&lt;value&gt;&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=nt>&lt;property&gt;</span>
            <span class=nt>&lt;name&gt;&lt;/name&gt;</span>
            <span class=nt>&lt;value&gt;&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
<span class=nt>&lt;/configuration&gt;</span>
</code></pre></td></tr></table></div></div><p>连接地址线程数计算：</p><p>这里写了一个python的小程序：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>math</span>

<span class=n>num</span><span class=o>=</span><span class=nb>int</span><span class=p>(</span><span class=nb>input</span> <span class=p>(</span><span class=s2>&#34;Enter the cluster node count: &#34;</span><span class=p>))</span>
<span class=n>th</span><span class=o>=</span><span class=p>(</span><span class=n>math</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>num</span><span class=p>)</span> <span class=o>*</span> <span class=mi>20</span><span class=p>)</span>
<span class=k>print</span> <span class=p>(</span><span class=s2>&#34;The dfs namnode handler count is &#34;</span><span class=p>,</span> <span class=nb>int</span><span class=p>(</span><span class=n>th</span><span class=p>))</span>
</code></pre></td></tr></table></div></div><p>关于这个参数的连接：</p><p><a href=https://blog.csdn.net/qq_43081842/article/details/102672420>https://blog.csdn.net/qq_43081842/article/details/102672420</a></p><p><code>fs.defaultFS：</code>NameNode地址
<code>hadoop.tmp.dir：</code>Hadoop临时目录</p><p><code>dfs.namenode.name.dir</code>：保存FSImage的目录，存放NameNode的<code>metadata</code>
<code>dfs.datanode.data.dir</code>：保存HDFS数据的目录，存放DataNode的多个数据块
<code>dfs.replication</code>：HDFS存储的临时备份数量，有两个Worker节点，因此数值为2</p><p>编辑<code>mapred-site.xml</code>配置文件：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>nano -w /opt/hadoop/etc/hadoop/mapred-site.xml
</code></pre></td></tr></table></div></div><p>内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-xml data-lang=xml><span class=nt>&lt;configuration&gt;</span>
	    <span class=c>&lt;!-- 指定MR使用框架为yarn --&gt;</span>
        <span class=nt>&lt;property&gt;</span>
                <span class=nt>&lt;name&gt;</span>mapreduce.framework.name<span class=nt>&lt;/name&gt;</span>
                <span class=nt>&lt;value&gt;</span>yarn<span class=nt>&lt;/value&gt;</span>
        <span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- 指定 MR Jobhistory的地址 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>mapreduce.jobhistory.address<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>hadoop-node1.nil.ml:10020<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
		<span class=c>&lt;!-- 任务历史服务器的web地址 --&gt;</span>
		<span class=nt>&lt;property&gt;</span>
			<span class=nt>&lt;name&gt;</span>mapreduce.jobhistory.webapp.address<span class=nt>&lt;/name&gt;</span>
			<span class=nt>&lt;value&gt;</span>hadoop-node1.nil.ml:19888<span class=nt>&lt;/value&gt;</span>
		<span class=nt>&lt;/property&gt;</span>
<span class=nt>&lt;/configuration&gt;</span>
</code></pre></td></tr></table></div></div><p>修改yarn配置文件<code>/opt/hadoop/etc/hadoop/yarn-site.xml </code>：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>nano <span class=nv>$HADOOP_HOME</span>/etc/hadoop/yarn-site.xml 
</code></pre></td></tr></table></div></div><p>内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-xml data-lang=xml><span class=nt>&lt;configuration&gt;</span> 
	<span class=c>&lt;!-- 开启MR高可用 --&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>yarn.resourcemanager.ha.enabled<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>true<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=c>&lt;!-- 指定RM的cluster id --&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span> yarn.resourcemanager.cluster-id<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>yrc<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=c>&lt;!-- 指定 RM的名字 --&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>yarn.resourcemanager.ha.rm-ids<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>rm1,rm2<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=c>&lt;!-- 分别指定RM的地址 --&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>yarn.resourcemanager.hostname.rm1<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>hadoop-node1.nil.ml<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>yarn.resourcemanager.webapp.address.rm1<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>hadoop-node1.nil.ml:8088<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>yarn.resourcemanager.webapp.address.rm2<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>hadoop-node2.nil.ml:8088<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>	
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>yarn.resourcemanager.hostname.rm2<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>hadoop-node2.nil.ml<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>	
	
	<span class=c>&lt;!-- 指定 zk集群的地址 --&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>hadoop.zk.address<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>hadoop-node1.nil.ml:2181,hadoop-node2.nil.ml:2181,hadoop-node3.nil.ml:2181<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
    <span class=c>&lt;!-- 指定 MR 走 shuffle --&gt;</span> 
	<span class=nt>&lt;property&gt;</span> 
		<span class=nt>&lt;name&gt;</span>yarn.nodemanager.aux-services<span class=nt>&lt;/name&gt;</span> 
		<span class=nt>&lt;value&gt;</span>mapreduce_shuffle<span class=nt>&lt;/value&gt;</span> 
	<span class=nt>&lt;/property&gt;</span> 
     <span class=nt>&lt;property&gt;</span>
         <span class=nt>&lt;name&gt;</span>yarn.log-aggregation-enable<span class=nt>&lt;/name&gt;</span>
         <span class=nt>&lt;value&gt;</span>true<span class=nt>&lt;/value&gt;</span>
     <span class=nt>&lt;/property&gt;</span>
     <span class=nt>&lt;property&gt;</span>
         <span class=nt>&lt;name&gt;</span>yarn.log-aggregation.retain-seconds<span class=nt>&lt;/name&gt;</span>
         <span class=nt>&lt;value&gt;</span>86400<span class=nt>&lt;/value&gt;</span>
    <span class=nt>&lt;/property&gt;</span>
	<span class=c>&lt;!-- 启用自动恢复 --&gt;</span>
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>yarn.resourcemanager.recovery.enabled<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>true<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
	<span class=c>&lt;!-- 指定 resourcemanager的状态信息存储在 zookeeper集群上 --&gt;</span>
	 <span class=nt>&lt;property&gt;</span>
         <span class=nt>&lt;name&gt;</span>yarn.resourcemanager.store.class<span class=nt>&lt;/name&gt;</span>
         <span class=nt>&lt;value&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class=nt>&lt;/value&gt;</span>
     <span class=nt>&lt;/property&gt;</span>
	<span class=c>&lt;!-- 环境变量的继承 --&gt;</span> 
	<span class=nt>&lt;property&gt;</span> 
		<span class=nt>&lt;name&gt;</span>yarn.nodemanager.env-whitelist<span class=nt>&lt;/name&gt;</span> 
		<span class=nt>&lt;value&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class=nt>&lt;/value&gt;</span> 
	<span class=nt>&lt;/property&gt;</span> 
	<span class=nt>&lt;property&gt;</span>
		<span class=nt>&lt;name&gt;</span>yarn.nodemanager.vmem-check-enabled<span class=nt>&lt;/name&gt;</span>
		<span class=nt>&lt;value&gt;</span>false<span class=nt>&lt;/value&gt;</span>
	<span class=nt>&lt;/property&gt;</span>
<span class=nt>&lt;/configuration&gt;</span>
</code></pre></td></tr></table></div></div><p>配置woker节点：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>vi /opt/hadoop/etc/hadoop/workers
</code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hadoop-node1.nil.ml
hadoop-node2.nil.ml
hadoop-node3.nil.ml
</code></pre></td></tr></table></div></div><p>复制文件到各个节点：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>rsync -avz /opt/hadoop -e ssh hadoop-node2.nil.ml:/opt/
rsync -avz /opt/hadoop -e ssh hadoop-node3.nil.ml:/opt/
</code></pre></td></tr></table></div></div><h3 id=启动>启动</h3><p>首先启动 journalnode（所有节点）</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hdfs --daemon start journalnode
</code></pre></td></tr></table></div></div><p>初始化namenode (hadoop-node1)节点：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hdfs namenode -format 
</code></pre></td></tr></table></div></div><p>复制文件到hadoop-node2节点上：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>cd</span> /opt/hdfs/namenode 
scp -r current hadoop-node2.nil.ml:<span class=nv>$PWD</span>
</code></pre></td></tr></table></div></div><p>格式化 zkfc</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hdfs zkfc -formatZK
</code></pre></td></tr></table></div></div><p>启动<code>hdfs</code></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>start-dfs.sh
</code></pre></td></tr></table></div></div><p>启动yarn</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>start-yarn.sh
</code></pre></td></tr></table></div></div><p>查看yarn状态：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>yarn rmadmin -getServiceState rm1
</code></pre></td></tr></table></div></div><p>手动启动 resourcemanager</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>yarn resourcemanager
</code></pre></td></tr></table></div></div><p>启动MR的任务历史服务器</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>mr-jobhistory-daemon.sh start historyserver
</code></pre></td></tr></table></div></div><p>查看HA节点状态：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hdfs haadmin -getServiceState nn1
</code></pre></td></tr></table></div></div><p>yarn</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>yarn rmadmin -getServiceState rm1
</code></pre></td></tr></table></div></div><p>网页：
HDFS
hadoop-node1.nil.ml:50070
hadoop-node2.nil.ml:50070</p><p>yarn
hadoop-node1.nil.ml:8088
hadoop-node2.nil.ml:8088</p><p>jobhistory</p><p>hadoop-node1.nil.ml:19888</p><h3 id=hadoop-服务-systemd-service-文件>hadoop 服务 systemd service 文件</h3><p>为了方便管理这里去创建几个systemd file用于去管理服务</p><p>hdfs</p><p>创建并编辑<code>/lib/systemd/system/hadoop-dfs.service</code></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo vi /lib/systemd/system/hadoop-dfs.service
</code></pre></td></tr></table></div></div><p>内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=o>[</span>Unit<span class=o>]</span>
<span class=nv>Description</span><span class=o>=</span>Hadoop DFS namenode and datanode
<span class=nv>After</span><span class=o>=</span>syslog.target network.target remote-fs.target nss-lookup.target network-online.target
<span class=nv>Requires</span><span class=o>=</span>network-online.target

<span class=o>[</span>Service<span class=o>]</span>
<span class=nv>User</span><span class=o>=</span>hdfs
<span class=nv>Group</span><span class=o>=</span>hadoop
<span class=nv>Type</span><span class=o>=</span>forking
<span class=nv>ExecStart</span><span class=o>=</span>/opt/hadoop/sbin/start-dfs.sh
<span class=nv>ExecStop</span><span class=o>=</span>/opt/hadoop/sbin/stop-dfs.sh
<span class=nv>WorkingDirectory</span><span class=o>=</span>/opt/hadoop/
<span class=nv>Environment</span><span class=o>=</span><span class=nv>JAVA_HOME</span><span class=o>=</span>/usr/lib/jvm/java-1.8.0-openjdk-amd64/
<span class=nv>Environment</span><span class=o>=</span><span class=nv>HADOOP_HOME</span><span class=o>=</span>/opt/hadoop/
<span class=nv>TimeoutStartSec</span><span class=o>=</span>2min
<span class=nv>Restart</span><span class=o>=</span>on-failure
<span class=nv>PIDFile</span><span class=o>=</span>/tmp/hadoop-hadoop-namenode.pid

<span class=o>[</span>Install<span class=o>]</span>
<span class=nv>WantedBy</span><span class=o>=</span>multi-user.target
</code></pre></td></tr></table></div></div><p>启动：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo systemctl start hadoop-dfs.service
</code></pre></td></tr></table></div></div><p>yarn</p><p>jobhistory</p><h3 id=hdfs-测试>HDFS 测试</h3><p>在hdfs上创建文件夹：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hdfs dfs -mkdir /test1
hdfs dfs -mkdir /logs 
</code></pre></td></tr></table></div></div><p>查看：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hdfs dfs -ls / 
</code></pre></td></tr></table></div></div><p>把系统的<code>/var/log/</code>下面的所有内容丢在我们创建的<code>/logs/</code>文件夹下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>hdfs dfs -put /var/log/* /logs/
</code></pre></td></tr></table></div></div><h3 id=ha验证>HA验证</h3><p>在这个章节我们将会测试之前部署hdfs HA是否可以正常使用</p><h4 id=干掉active-namenode>干掉active namenode</h4><p>首先我们干掉active namenode然后看看集群有什么变化</p><h3 id=为每个服务编写systemd-services-文件>为每个服务编写systemd services 文件</h3><h4 id=zookeeper>zookeeper</h4><h4 id=hadoop-hdfs>hadoop hdfs</h4><h4 id=hadoop-yarn>hadoop yarn</h4><h2 id=海豚调度>海豚调度</h2><h3 id=安装数据库>安装数据库</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo apt-get update <span class=o>&amp;&amp;</span> sudo apt-get install wget -y
</code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo apt install mysql-server
</code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo mysql_secure_installation
</code></pre></td></tr></table></div></div><h3 id=安装和配置海豚调度>安装和配置海豚调度</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>tar -xf apache-dolphinscheduler-1.3.6-bin.tar.g
</code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo mv apache-dolphinscheduler-1.3.6-bin /opt/dolphinscheduler-bin

</code></pre></td></tr></table></div></div><p>创建用户：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo useradd -m -g sudo dolphinscheduler -s /bin/bash
<span class=nb>echo</span> <span class=s2>&#34;dolphinscheduler:changeme&#34;</span> <span class=p>|</span> sudo chpasswd
</code></pre></td></tr></table></div></div><p>修改权限：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo chown -R dolphinscheduler /opt/dolphinscheduler-bin
</code></pre></td></tr></table></div></div><p>切换用户：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo su - dolphinscheduler
</code></pre></td></tr></table></div></div><p>配置ssh免密：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>su dolphinscheduler<span class=p>;</span>

ssh-keygen -t rsa -P <span class=s1>&#39;&#39;</span> -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
chmod <span class=m>600</span> ~/.ssh/authorized_keys
</code></pre></td></tr></table></div></div><p>更改默认的数据库密码规则：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-sql data-lang=sql><span class=k>show</span> <span class=n>VARIABLES</span> <span class=k>like</span> <span class=s2>&#34;%password%&#34;</span><span class=p>;</span>
</code></pre></td></tr></table></div></div><p>这里可以看到输出的<code>validate_password.policy</code>字段的值为<code>MEDIUM</code>中等的密码强度，根据需要更改为不同的值。</p><p>这里修改为最低的密码等级（仅用于测试）。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-sql data-lang=sql><span class=k>SET</span> <span class=n>PERSIST</span> <span class=n>valicate_password</span><span class=p>.</span><span class=n>policy</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</code></pre></td></tr></table></div></div><p>数据库初始化：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-sql data-lang=sql><span class=k>CREATE</span> <span class=k>DATABASE</span> <span class=n>dolphinscheduler</span> <span class=k>DEFAULT</span> <span class=nb>CHARACTER</span> <span class=k>SET</span> <span class=n>utf8</span> <span class=k>DEFAULT</span> <span class=k>COLLATE</span> <span class=n>utf8_general_ci</span><span class=p>;</span>
<span class=k>CREATE</span> <span class=k>USER</span> <span class=s1>&#39;dolphinscheduler&#39;</span><span class=o>@</span><span class=s1>&#39;localhost&#39;</span> <span class=n>IDENTIFIED</span> <span class=k>BY</span> <span class=s1>&#39;changeme&#39;</span><span class=p>;</span>
<span class=n>ES</span> <span class=k>ON</span> <span class=n>dolphinscheduler</span><span class=p>.</span><span class=o>*</span> <span class=k>TO</span> <span class=s1>&#39;dolphinscheduler&#39;</span><span class=o>@</span><span class=s1>&#39;localhost&#39;</span><span class=p>;</span>
<span class=n>flush</span> <span class=k>privileges</span><span class=p>;</span>
</code></pre></td></tr></table></div></div><p>创建表和导入基础数据</p><p>首先安装mysql的连接器：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>wget -c https://downloads.mysql.com/archives/get/p/3/file/mysql-connector-java_8.0.24-1ubuntu18.04_all.deb
</code></pre></td></tr></table></div></div><p>安装：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo dpkg -i mysql-connector-java_8.0.24-1ubuntu18.04_all.deb
</code></pre></td></tr></table></div></div><p>复制库文件到<code>dolphinscheduler</code>的<code>lib</code>目录下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>cp -v /usr/share/java/mysql-connector-java-8.0.24.jar /opt/dolphinscheduler-bin/lib/
</code></pre></td></tr></table></div></div><p>修改<code>conf</code>目录下的<code>datasource.properties</code>中的配置：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>vi conf/datasource.properties
</code></pre></td></tr></table></div></div><p>注释掉 postgresql的部分</p><p>取消注释掉 mysql部分的，内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>spring.datasource.driver-class-name<span class=o>=</span>com.mysql.jdbc.Driver
spring.datasource.url<span class=o>=</span>jdbc:mysql://localhost:3306/dolphinscheduler?useUnicode<span class=o>=</span>true<span class=p>&amp;</span><span class=nv>characterEncoding</span><span class=o>=</span>UTF-8<span class=p>&amp;</span><span class=nv>allowMultiQueries</span><span class=o>=</span><span class=nb>true</span>
spring.datasource.username<span class=o>=</span>dolphinscheduler
spring.datasource.password<span class=o>=</span>changeme
</code></pre></td></tr></table></div></div><p>执行<code>script</code>目录下的数据库脚本：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sh script/create-dolphinscheduler.sh
</code></pre></td></tr></table></div></div><p>修改运行参数：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>vi conf/env/dolphinscheduler_env.sh
</code></pre></td></tr></table></div></div><p>内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=nb>export</span> <span class=nv>HADOOP_HOME</span><span class=o>=</span>/opt/hadoop
<span class=nb>export</span> <span class=nv>HADOOP_CONF_DIR</span><span class=o>=</span>/opt/hadoop/etc/hadoop
<span class=nb>export</span> <span class=nv>JAVA_HOME</span><span class=o>=</span>/usr/lib/jvm/java-1.8.0-openjdk-amd64/
<span class=nb>export</span> <span class=nv>PATH</span><span class=o>=</span><span class=nv>$HADOOP_HOME</span>/bin:<span class=nv>$JAVA_HOME</span>/bin:<span class=nv>$PATH</span>
</code></pre></td></tr></table></div></div><p>修改一键部署脚本：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>vi conf/config/install_config.conf
</code></pre></td></tr></table></div></div><p>内容如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span><span class=lnt>78
</span><span class=lnt>79
</span><span class=lnt>80
</span><span class=lnt>81
</span><span class=lnt>82
</span><span class=lnt>83
</span><span class=lnt>84
</span><span class=lnt>85
</span><span class=lnt>86
</span><span class=lnt>87
</span><span class=lnt>88
</span><span class=lnt>89
</span><span class=lnt>90
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell><span class=c1># 这里填 mysql or postgresql</span>
<span class=nv>dbtype</span><span class=o>=</span><span class=s2>&#34;mysql&#34;</span>

<span class=c1># 数据库连接地址</span>
<span class=nv>dbhost</span><span class=o>=</span><span class=s2>&#34;localhost:3306&#34;</span>

<span class=c1># 数据库名</span>
<span class=nv>dbname</span><span class=o>=</span><span class=s2>&#34;dolphinscheduler&#34;</span>

<span class=c1># 数据库用户名，此处需要修改为上面设置的{user}具体值</span>
<span class=nv>username</span><span class=o>=</span><span class=s2>&#34;dolphinscheduler&#34;</span>    

<span class=c1># 数据库密码, 如果有特殊字符，请使用\转义，需要修改为上面设置的{password}具体值</span>
<span class=nv>password</span><span class=o>=</span><span class=s2>&#34;changeme&#34;</span>

<span class=c1>#Zookeeper地址，单机本机是localhost:2181，记得把2181端口带上</span>
<span class=nv>zkQuorum</span><span class=o>=</span><span class=s2>&#34;hadoop-node1.nil.ml:2181,hadoop-node2.nil.ml:2181,hadoop-node3.nil.ml:2181&#34;</span>

<span class=c1>#将DS安装到哪个目录，如: /opt/soft/dolphinscheduler，不同于现在的目录</span>
<span class=nv>installPath</span><span class=o>=</span><span class=s2>&#34;/opt/dolphinscheduler&#34;</span>

<span class=c1>#使用哪个用户部署，使用第3节创建的用户</span>
<span class=nv>deployUser</span><span class=o>=</span><span class=s2>&#34;dolphinscheduler&#34;</span>

<span class=c1># 邮件配置，以qq邮箱为例</span>
<span class=c1># 邮件协议</span>
<span class=nv>mailProtocol</span><span class=o>=</span><span class=s2>&#34;SMTP&#34;</span>

<span class=c1># 邮件服务地址</span>
<span class=nv>mailServerHost</span><span class=o>=</span><span class=s2>&#34;smtp.qq.com&#34;</span>

<span class=c1># 邮件服务端口</span>
<span class=nv>mailServerPort</span><span class=o>=</span><span class=s2>&#34;25&#34;</span>

<span class=c1># mailSender和mailUser配置成一样即可</span>
<span class=c1># 发送者</span>
<span class=nv>mailSender</span><span class=o>=</span><span class=s2>&#34;xxx@qq.com&#34;</span>

<span class=c1># 发送用户</span>
<span class=nv>mailUser</span><span class=o>=</span><span class=s2>&#34;xxx@qq.com&#34;</span>

<span class=c1># 邮箱密码</span>
<span class=nv>mailPassword</span><span class=o>=</span><span class=s2>&#34;xxx&#34;</span>

<span class=c1># TLS协议的邮箱设置为true，否则设置为false</span>
<span class=nv>starttlsEnable</span><span class=o>=</span><span class=s2>&#34;true&#34;</span>

<span class=c1># 开启SSL协议的邮箱配置为true，否则为false。注意: starttlsEnable和sslEnable不能同时为true</span>
<span class=nv>sslEnable</span><span class=o>=</span><span class=s2>&#34;false&#34;</span>

<span class=c1># 邮件服务地址值，参考上面 mailServerHost</span>
<span class=nv>sslTrust</span><span class=o>=</span><span class=s2>&#34;smtp.qq.com&#34;</span>

<span class=c1># 业务用到的比如sql等资源文件上传到哪里，可以设置：HDFS,S3,NONE，单机如果想使用本地文件系统，请配置为HDFS，因为HDFS支持本地文件系统；如果不需要资源上传功能请选择NONE。强调一点：使用本地文件系统不需要部署hadoop</span>
<span class=nv>resourceStorageType</span><span class=o>=</span><span class=s2>&#34;HDFS&#34;</span>

<span class=c1># 这里以保存到本地文件系统为例</span>
<span class=c1>#注：但是如果你想上传到HDFS的话，NameNode启用了HA，则需要将hadoop的配置文件core-site.xml和hdfs-site.xml放到conf目录下，本例即是放到/opt/dolphinscheduler/conf下面，并配置namenode cluster名称；如果NameNode不是HA,则修改为具体的ip或者主机名即可</span>
<span class=nv>defaultFS</span><span class=o>=</span><span class=s2>&#34;file:///hadoop-cluster/dolphinscheduler&#34;</span>    <span class=c1>#hdfs://{具体的ip/主机名}:8020</span>

<span class=c1># 如果没有使用到Yarn,保持以下默认值即可；如果ResourceManager是HA，则配置为ResourceManager节点的主备ip或者hostname,比如&#34;192.168.xx.xx,192.168.xx.xx&#34;;如果是单ResourceManager请配置yarnHaIps=&#34;&#34;即可</span>
<span class=c1># 注：依赖于yarn执行的任务，为了保证执行结果判断成功,需要确保yarn信息配置正确。</span>
<span class=nv>yarnHaIps</span><span class=o>=</span><span class=s2>&#34;hadoop-node1.nil.ml,hadoop-node2.nil.ml,hadoop-node3.nil.ml&#34;</span>

<span class=c1># 如果ResourceManager是HA或者没有使用到Yarn保持默认值即可；如果是单ResourceManager，请配置真实的ResourceManager主机名或者ip</span>
<span class=nv>singleYarnIp</span><span class=o>=</span><span class=s2>&#34;yrc&#34;</span>

<span class=c1># 资源上传根路径,支持HDFS和S3,由于hdfs支持本地文件系统，需要确保本地文件夹存在且有读写权限</span>
<span class=nv>resourceUploadPath</span><span class=o>=</span><span class=s2>&#34;/data/dolphinscheduler/&#34;</span>

<span class=c1># 具备权限创建resourceUploadPath的用户</span>
<span class=nv>hdfsRootUser</span><span class=o>=</span><span class=s2>&#34;hdfs&#34;</span>

<span class=c1>#在哪些机器上部署DS服务，本机选localhost</span>
<span class=nv>ips</span><span class=o>=</span><span class=s2>&#34;localhost&#34;</span>

<span class=c1>#ssh端口,默认22</span>
<span class=nv>sshPort</span><span class=o>=</span><span class=s2>&#34;22&#34;</span>

<span class=c1>#master服务部署在哪台机器上</span>
<span class=nv>masters</span><span class=o>=</span><span class=s2>&#34;localhost&#34;</span>

<span class=c1>#worker服务部署在哪台机器上,并指定此worker属于哪一个worker组,下面示例的default即为组名</span>
<span class=nv>workers</span><span class=o>=</span><span class=s2>&#34;localhost:default&#34;</span>

<span class=c1>#报警服务部署在哪台机器上</span>
<span class=nv>alertServer</span><span class=o>=</span><span class=s2>&#34;localhost&#34;</span>

<span class=c1>#后端api服务部署在在哪台机器上</span>
<span class=nv>apiServers</span><span class=o>=</span><span class=s2>&#34;localhost&#34;</span>
</code></pre></td></tr></table></div></div><p>创建目录：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo mkdir -pv /data/dolphinscheduler
sudo chown -R dolphinscheduler:dolphinscheduler /data/dolphinscheduler
</code></pre></td></tr></table></div></div><p>在部署之前先切换一下默认的 <code>/bin/sh</code>连接位置：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo rm /bin/sh
sudo ln -s /usr/bin/bash /bin/sh
</code></pre></td></tr></table></div></div><p>一键部署：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sh install.sh
</code></pre></td></tr></table></div></div><p>登录地址：hadoop-node1.nil.ml:12345/dolphinscheduler
默认用户：admin
默认密码：dolphinscheduler123</p><h2 id=日志处理>日志处理</h2><h2 id=安全加固>安全加固</h2><p>kerberos 认证开启</p><p>规划各个服务的分配kerberos的principal</p><p>首先在每个节点上执行：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>sudo apt-get install krb5-user -y
mkdir -pv /etc/security/keytabs
</code></pre></td></tr></table></div></div><p>我们预计使用Kerberos的服务有：</p><p>namenode, secondarynamenode, datanode, resourmanager, nodemanager, job historyserver, https, hive</p><p>比如说 namenode 运行在<code>hadoop-node1.nil.ml</code>和<code>hadoop-node2.nil.ml</code>上对应分配的帐号为：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>nn/hadoop-node1.nil.ml@NIL.ML
</code></pre></td></tr></table></div></div><p>其中<code>nn</code>表示为<code>namenode</code>,<code>hadoop-node1.nil.ml</code>是<code>principal</code>中的<code>instance</code>，最后是域<code>NIL.ML</code></p><p>其他服务也类似的：</p><table><thead><tr><th>账户名称</th><th>主机名</th><th>域</th></tr></thead><tbody><tr><td>nn</td><td>hadoop-node1.nil.ml</td><td>NIL.ML</td></tr><tr><td>nn</td><td>hadoop-node2.nil.ml</td><td>NIL.ML</td></tr><tr><td>sn</td><td>hadoop-node1.nil.ml</td><td>NIL.ML</td></tr><tr><td>http</td><td>hadoop-node1.nil.ml</td><td>NIL.ML</td></tr><tr><td></td><td></td><td>NIL.ML</td></tr><tr><td></td><td></td><td>NIL.ML</td></tr></tbody></table><p>登录到 kerberos的管理后台：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>kadmin
</code></pre></td></tr></table></div></div><p>创建<code>namenode</code>的账户和对应的keytab：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>addprinc -randkey nn/hadoop-node1.nil.ml@NIL.ML
ktadd -k /etc/security/keytabs/nn.services.keytab nn/hadoop-node1.nil.ml@NIL.ML
</code></pre></td></tr></table></div></div><p>创建<code>secondarynamenode</code>的账户和对应的keytab：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>addprinc -randkey sn/hadoop-node1.nil.ml@NIL.ML
ktadd -k /etc/security/keytabs/sn.services.keytab sn/hadoop-node1.nil.ml@NIL.ML
</code></pre></td></tr></table></div></div><p>创建用于https的账户和对应的keytab：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-shell data-lang=shell>addprinc -randkey http/hadoop-node1.nil.ml@NIL.ML
ktadd -k /etc/security/keytabs/http.services.keytab http/hadoop-node1.nil.ml@NIL.ML
</code></pre></td></tr></table></div></div><p>更改<code>keytab</code>文件权限为 400 只允许所有者可以读
http 440 组用户也允许读</p><h2 id=后记>后记</h2><h2 id=参考链接>参考链接</h2><p>(How to Install and Configure Hadoop on Ubuntu 20.04)[https://tecadmin.net/install-hadoop-on-ubuntu-20-04/]
(Configuring Ports)[https://ambari.apache.org/1.2.3/installing-hadoop-using-ambari/content/reference_chap2.html]
<a href=https://developpaper.com/building-hadoop-high-availability-cluster-based-on-zookeeper/>https://developpaper.com/building-hadoop-high-availability-cluster-based-on-zookeeper/</a>
<a href=https://tutorials.freshersnwo.com/hadoop-tutorial/hadoop-high-availability/>https://tutorials.freshersnwo.com/hadoop-tutorial/hadoop-high-availability/</a>
<a href=https://www.xenonstack.com/insights/apache-zookeeper-security/>https://www.xenonstack.com/insights/apache-zookeeper-security/</a></p><p><a href=https://zookeeper.apache.org/doc/r3.6.0/zookeeperAdmin.html#sc_authOptions>https://zookeeper.apache.org/doc/r3.6.0/zookeeperAdmin.html#sc_authOptions</a></p><p><a href=https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/SecureContainer.html>https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/SecureContainer.html</a></p><p><a href=https://docs.confluent.io/platform/current/security/zk-security.html>https://docs.confluent.io/platform/current/security/zk-security.html</a></p><p><a href=https://docs.cloudera.com/runtime/7.2.0/zookeeper-security/topics/zookeeper-configure-client-shell-kerberos-authentication.html>https://docs.cloudera.com/runtime/7.2.0/zookeeper-security/topics/zookeeper-configure-client-shell-kerberos-authentication.html</a></p><p><a href=https://www.xenonstack.com/insights/apache-zookeeper-security/>https://www.xenonstack.com/insights/apache-zookeeper-security/</a>
<a href=https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SecureMode.html>https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SecureMode.html</a></p></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>Yafa Xena</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2021-02-01</span></p></div><footer class=post-footer><div class=post-tags><a href=/tags/hadoop/>hadoop</a>
<a href=/tags/linux/>linux</a></div><nav class=post-nav><a class=prev href=/post/2020-summary/><i class="iconfont icon-left"></i><span class="prev-text nav-default">2020年终总结</span>
<span class="prev-text nav-mobile">Prev</span></a>
<a class=next href=/post/setup-my-lab/><span class="next-text nav-default">设置我的实验环境</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><script src=https://utteranc.es/client.js repo=yafa-xena/yafa-xena/blog-comments issue-term=pathname theme=github-light crossorigin=anonymous async></script><noscript>Please enable JavaScript to view the <a href=https://github.com/utterance>comments powered by utterances.</a></noscript></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:i@yafa.moe class="iconfont icon-email" title=email></a><a href=https://twitter.com/XenaYafa class="iconfont icon-twitter" title=twitter></a><a href=https://github.com/yafa-xena/ class="iconfont icon-github" title=github></a><a href=https://www.yafa.moe/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span>
<span class=copyright-year>&copy;
2021<span class=heart><i class="iconfont icon-heart"></i></span><span>Yafa Xena</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=/js/main.min.c12618f9a600c40bd024996677e951e64d3487006775aeb22e200c990006c5c7.js></script><script type=text/javascript>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],tags:'ams',}};</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-194993895-1','auto');ga('set','anonymizeIp',true);ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></body></html>